# 30 задач для изучения библиотеки pandas

## Примерная общая структура каждой задачи:

Каждая задача должна содержать 45 пунктов по следующей структуре:

1. **Загрузка (6 пунктов)**: загрузка, просмотр, info, describe, columns, shape
2. **Пропуски и дубликаты (6 пунктов)**: подсчет, проценты, заполнение, проверка, удаление
3. **Типы данных (3 пункта)**: преобразование типов
4. **Новые признаки (6 пунктов)**: создание столбцов
5. **Фильтрация (7 пунктов)**: фильтры, query, сортировка, топ-N
6. **Группировка (6 пунктов)**: groupby, agg, статистика
7. **Сводные таблицы (4 пункта)**: pivot_table, crosstab, margins
8. **Объединение (5 пунктов)**: merge, concat, join
9. **Сохранение (2 пункта)**: CSV и Excel

---

## Итоговая сводка по всем 30 задачам

### Задачи 1-10: Базовый уровень
- Задача 1: UK Police Crime Data (юридическая)
- Задача 2: Titanic Dataset
- Задача 3: World Happiness Report
- Задача 4: Students Performance
- Задача 5: Supermarket Sales
- Задача 6: Bike Sharing
- Задача 7: Wine Quality
- Задача 8: Adult Income
- Задача 9: Student Performance (merge задач)
- Задача 10: Breast Cancer Wisconsin

### Задачи 11-20: Средний уровень
- Задача 11: Iris Classification
- Задача 12: Car Evaluation
- Задача 13: Bank Marketing
- Задача 14: Mushroom Classification
- Задача 15: Zoo Animals
- Задача 16: Heart Disease
- Задача 17: Diabetes
- Задача 18: FIFA Players
- Задача 19: COVID-19
- Задача 20: Olympic History

### Задачи 21-30: Продвинутый уровень
- Задача 21: Federal Court Cases (юридическая)
- Задача 22: Recidivism Analysis (юридическая)
- Задача 23: Real Estate Valuation
- Задача 24: Air Quality
- Задача 25: Online Retail
- Задача 26: Employee Attrition
- Задача 27: Credit Card Fraud
- Задача 28: Stock Market
- Задача 29: Flight Delays
- Задача 30: Multi-Source Integration (комплексная)

---

## Дополнительные рекомендации

### Для всех задач используйте:
- `pd.read_csv()` для загрузки
- `df.head()`, `df.tail()`, `df.sample()`
- `df.info()`, `df.describe()`, `df.shape`
- `df.isnull().sum()`, `df.duplicated()`
- `df.fillna()`, `df.dropna()`, `df.drop_duplicates()`
- `df.astype()`, `pd.to_datetime()`
- `df.groupby()`, `df.agg()`
- `df.pivot_table()`, `pd.crosstab()`
- `pd.merge()`, `pd.concat()`, `df.join()`
- `df.to_csv()`, `df.to_excel()`

### Все датасеты доступны по прямым ссылкам и имеют открытые лицензии!


## Задача 1: Анализ преступности в Великобритании

**Датасет**: UK Police Street Level Crime Data  
**Источник**: https://data.police.uk/  
**Прямая ссылка**: https://data.police.uk/data/archive/2024-08.zip (выберите файл для одной полицейской территории)  
**Лицензия**: Open Government Licence v3.0

**Описание**: Датасет содержит информацию о преступлениях на уровне улиц: тип преступления, локация, результат расследования. Столбцы включают: Crime ID, Month, Reported by, Falls within, Longitude, Latitude, Location, LSOA code, LSOA name, Crime type, Last outcome category, Context.

### Загрузка и первичный анализ

1. Загрузите CSV файл с данными о преступлениях
2. Выведите первые 15 строк датасета
3. Выведите информацию о структуре данных (типы столбцов, количество записей)
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в столбце `Context` строкой 'No context'
10. Проверьте наличие полных дубликатов в датасете
11. Проверьте наличие дубликатов по столбцу `Crime ID`
12. Удалите строки, где `Crime ID` пустой

### Преобразование типов данных

13. Преобразуйте столбец `Month` в формат datetime
14. Проверьте, что преобразование выполнено корректно
15. Преобразуйте `Longitude` и `Latitude` в тип float

### Создание новых признаков

16. Извлеките год из столбца `Month` в отдельный столбец `year`
17. Извлеките месяц (название) в столбец `month_name`
18. Создайте бинарный столбец `has_outcome` (True, если `Last outcome category` не пустой)
19. Создайте столбец `crime_category` группируя похожие типы преступлений
20. Создайте бинарный столбец `is_violent` для насильственных преступлений
21. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

22. Отфильтруйте только преступления типа 'Violence and sexual offences'
23. Сохраните результат в переменную `violent_crimes`
24. Отфильтруйте преступления с известным исходом (has_outcome == True)
25. Используя метод `query()`, найдите преступления: `Latitude > 51.5`
26. Отсортируйте датасет по `Crime type` (алфавитный порядок)
27. Отсортируйте по `LSOA name`, затем по `Crime type`
28. Найдите топ-20 локаций (`Location`) с наибольшим количеством преступлений

### Группировка и агрегация

29. Сгруппируйте данные по `Crime type` и подсчитайте количество преступлений
30. Для каждого типа преступления вычислите процент дел с известным исходом
31. Найдите тип преступления с наибольшим количеством случаев
32. Сгруппируйте по `LSOA name` и вычислите:
    - Количество преступлений
    - Количество уникальных типов преступлений
    - Процент дел с известным исходом
33. Используя метод `agg()`, для каждого `Crime type` вычислите:
```python
{
    'Crime ID': 'count',
    'has_outcome': 'mean'
}
```
34. Найдите топ-10 районов (`LSOA name`) по количеству преступлений

### Сводные таблицы

35. Создайте сводную таблицу: `month_name` в строках, топ-5 `Crime type` в столбцах
36. Создайте сводную таблицу с процентом раскрываемости: `LSOA name` × `Crime type`
37. Добавьте итоговые суммы к сводной таблице (параметр `margins=True`)
38. Создайте таблицу `crosstab` для `Crime type` и `has_outcome`

### Объединение датафреймов

39. Создайте датафрейм `crime_stats` с топ-10 типов преступлений
40. Создайте датафрейм `area_stats` со статистикой по районам
41. Объедините основной датасет и `crime_stats` используя `merge()`
42. Создайте два датафрейма: один с исходами, другой без
43. Объедините их обратно используя `pd.concat()` с параметром `ignore_index=True`

### Сохранение результатов

44. Сохраните результаты в файл `uk_crime_analysis.csv`
45. Сохраните в Excel со следующими листами:
    - 'Raw_Data': исходные данные (первые 1000 строк)
    - 'By_Crime_Type': группировка по типам
    - 'By_Area': группировка по районам
    - 'Pivot': сводные таблицы

---

## Задача 2: Анализ пассажиров Титаника

**Датасет**: Titanic Dataset  
**Источник**: Stanford CS109  
**Прямая ссылка**: https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv  
**Лицензия**: Public Domain

**Описание**: Классический датасет с информацией о пассажирах Титаника. Столбцы: Survived, Pclass, Name, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard, Fare.

### Загрузка и первичный анализ

1. Загрузите датасет Titanic
2. Выведите первые 10 строк
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в столбце `Age` медианным значением
10. Проверьте наличие полных дубликатов
11. Удалите строки с пропущенным значением `Fare`
12. Проверьте, остались ли пропуски

### Преобразование типов данных

13. Преобразуйте столбец `Survived` в булевый тип
14. Убедитесь, что `Age` имеет тип float
15. Преобразуйте `Pclass` в категориальный тип

### Создание новых признаков

16. Создайте столбец `family_size` = `Siblings/Spouses Aboard` + `Parents/Children Aboard`
17. Создайте столбец `is_alone` (True, если `family_size` == 0)
18. Создайте столбец `age_group`: child (<18), adult (18-60), senior (>60)
19. Создайте столбец `fare_category`: cheap (<10), medium (10-30), expensive (>30)
20. Извлеките титул (Mr., Mrs., Miss. и т.д.) из столбца `Name` в новый столбец `title`
21. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

22. Отфильтруйте пассажиров первого класса
23. Сохраните результат в переменную `first_class`
24. Отфильтруйте выживших женщин
25. Используя метод `query()`, найдите: `Pclass == 3 and Age < 18`
26. Отсортируйте по `Fare` (от большего к меньшему)
27. Отсортируйте по `Pclass`, затем по `Age`
28. Найдите топ-20 пассажиров с самым дорогим билетом

### Группировка и агрегация

29. Сгруппируйте по `Pclass` и подсчитайте количество пассажиров
30. Для каждого класса вычислите процент выживших
31. Найдите класс с наибольшей выживаемостью
32. Сгруппируйте по `Sex` и вычислите:
    - Количество пассажиров
    - Средний возраст
    - Процент выживших
33. Используя метод `agg()`, для каждого `Pclass` вычислите:
```python
{
    'Age': ['mean', 'min', 'max'],
    'Fare': ['mean', 'median'],
    'Survived': 'mean'
}
```
34. Найдите топ-5 `title` по количеству выживших

### Сводные таблицы

35. Создайте сводную таблицу: `Pclass` в строках, `Sex` в столбцах, процент выживших
36. Создайте сводную таблицу со средним `Fare`: `age_group` × `Pclass`
37. Добавьте итоговые значения (параметр `margins=True`)
38. Создайте `crosstab` для `Pclass` и `Survived`

### Объединение датафреймов

39. Создайте датафрейм `class_stats` со статистикой по классам
40. Создайте датафрейм `survival_by_age` со статистикой по возрастным группам
41. Объедините основной датасет и `class_stats` по `Pclass` (left join)
42. Разделите на выживших и погибших
43. Объедините обратно используя `pd.concat()`

### Сохранение результатов

44. Сохраните очищенный датасет в `titanic_clean.csv`
45. Сохраните анализ в Excel с листами:
    - 'Clean_Data': очищенные данные
    - 'Statistics': описательная статистика
    - 'By_Class': группировка по классам
    - 'By_Sex': группировка по полу

---

## Задача 3: World Happiness Report

**Датасет**: World Happiness Report 2019  
**Источник**: Kaggle (UNSDSN)  
**Прямая ссылка**: https://www.kaggle.com/datasets/unsdsn/world-happiness (скачайте 2019.csv)  
**Альтернатива**: https://happiness-report.s3.amazonaws.com/2019/Chapter2OnlineData.xls  
**Лицензия**: CC0 Public Domain

**Описание**: Данные о счастье в 156 странах. Столбцы: Country, Score, GDP per capita, Social support, Healthy life expectancy, Freedom to make life choices, Generosity, Perceptions of corruption.

### Загрузка и первичный анализ

1. Загрузите датасет World Happiness Report 2019
2. Выведите первые 10 строк
3. Выведите информацию о структуре данных
4. Выведите описательную статистику
5. Выведите список всех колонок
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте пропущенные значения
8. Вычислите процент пропусков
9. Заполните пропуски в `Generosity` медианой
10. Проверьте дубликаты по `Country`
11. Удалите полные дубликаты
12. Проверьте финальное количество строк

### Преобразование типов данных

13. Убедитесь, что все метрики имеют тип float
14. Преобразуйте `Country` в строковый тип
15. Создайте индекс из `Country`

### Создание новых признаков

16. Создайте `happiness_rank` на основе `Score`
17. Создайте `happiness_category`: Low (<5), Medium (5-7), High (>7)
18. Создайте `gdp_category`: Low, Medium, High на основе квартилей
19. Создайте `support_index` = `Social support` + `Freedom to make life choices`
20. Создайте `negative_factors` = `Perceptions of corruption` (нормализовать)
21. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

22. Отфильтруйте страны с `Score` > 7
23. Сохраните в `happiest_countries`
24. Отфильтруйте страны с высоким GDP
25. Используя `query()`, найдите: `Score > 6 and GDP_per_capita > 1.0`
26. Отсортируйте по `Score` (от большего к меньшему)
27. Отсортируйте по `GDP per capita`, затем по `Score`
28. Найдите топ-20 стран по `Freedom to make life choices`

### Группировка и агрегация

29. Сгруппируйте по `happiness_category` и подсчитайте количество стран
30. Для каждой категории вычислите средний GDP
31. Найдите категорию с максимальным `Social support`
32. Сгруппируйте по `gdp_category` и вычислите:
    - Среднее Score
    - Среднее Social support
    - Среднее Freedom
33. Используя `agg()`, для каждой `happiness_category` вычислите:
```python
{
    'Score': ['mean', 'std'],
    'GDP per capita': ['mean', 'min', 'max'],
    'Social support': 'mean'
}
```
34. Найдите топ-10 стран по `Healthy life expectancy`

### Сводные таблицы

35. Создайте сводную таблицу: `happiness_category` × `gdp_category` с количеством стран
36. Создайте сводную таблицу со средним `Score`
37. Добавьте итоги с `margins=True`
38. Создайте `crosstab` для категорий счастья и GDP

### Объединение датафреймов

39. Создайте `top_countries` с топ-20 стран
40. Создайте `bottom_countries` с последними 20 странами
41. Объедините их с основным датасетом
42. Создайте датафрейм с региональными данными (если доступен)
43. Объедините используя `pd.concat()`

### Сохранение результатов

44. Сохраните в `happiness_analysis.csv`
45. Сохраните в Excel:
    - 'All_Countries': все данные
    - 'Top_20': топ-20 стран
    - 'Statistics': статистика по категориям
    - 'Correlations': корреляционная матрица

---

## Задача 4: Успеваемость студентов

**Датасет**: Students Performance in Exams  
**Источник**: Kaggle  
**Прямая ссылка**: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams  
**Альтернатива**: https://github.com/datasciencedojo/datasets/blob/master/StudentsPerformance.csv  
**Лицензия**: CC0 Public Domain

**Описание**: Оценки 1000 студентов по математике, чтению и письму с демографическими данными. Столбцы: gender, race/ethnicity, parental level of education, lunch, test preparation course, math score, reading score, writing score.

### Загрузка и первичный анализ

1. Загрузите датасет
2. Выведите первые 10 строк
3. Выведите информацию о структуре
4. Выведите описательную статистику
5. Выведите список колонок
6. Проверьте размер

### Обработка пропусков и дубликатов

7. Подсчитайте пропуски
8. Вычислите процент пропусков
9. Проверьте, есть ли пропуски (их не должно быть)
10. Проверьте полные дубликаты
11. Проверьте дубликаты по всем столбцам оценок
12. Удалите дубликаты если есть

### Преобразование типов данных

13. Преобразуйте `test preparation course` в бинарный (0/1)
14. Преобразуйте категориальные столбцы в category type
15. Убедитесь, что оценки имеют тип int

### Создание новых признаков

16. Создайте `average_score` = среднее из трех оценок
17. Создайте `total_score` = сумма всех оценок
18. Создайте `passed_all` (True если все оценки >= 60)
19. Создайте `performance_category`: Poor (<50), Average (50-70), Good (70-85), Excellent (>85)
20. Создайте `math_level`: Below Average, Average, Above Average
21. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

22. Отфильтруйте студентов с `average_score` > 80
23. Сохраните в `top_students`
24. Отфильтруйте студентов, прошедших test preparation
25. Используя `query()`: `math_score > 80 and reading_score > 80`
26. Отсортируйте по `average_score` (убывание)
27. Отсортируйте по `gender`, затем по `average_score`
28. Найдите топ-20 по `math score`

### Группировка и агрегация

29. Сгруппируйте по `gender` и подсчитайте количество
30. Для каждого пола вычислите средние оценки
31. Найдите пол с лучшими результатами
32. Сгруппируйте по `parental level of education` и вычислите:
    - Среднюю оценку по всем предметам
    - Процент passed_all
    - Количество студентов
33. Используя `agg()`, для `test preparation course`:
```python
{
    'math score': ['mean', 'std'],
    'reading score': ['mean', 'std'],
    'writing score': ['mean', 'std']
}
```
34. Найдите топ-3 `parental level of education` по average_score

### Сводные таблицы

35. Создайте сводную: `gender` × `test preparation course` с average_score
36. Создайте сводную: `lunch` × `parental level of education` с процентом passed_all
37. Добавьте итоги
38. Создайте `crosstab` для `gender` и `performance_category`

### Объединение датафреймов

39. Создайте `gender_stats` со статистикой по полу
40. Создайте `prep_stats` со статистикой по подготовке
41. Объедините основной датасет с `gender_stats`
42. Разделите на тех, кто прошел подготовку и нет
43. Объедините обратно

### Сохранение результатов

44. Сохраните в `students_analysis.csv`
45. Сохраните в Excel:
    - 'Students': все данные
    - 'Top_Performers': топ-100
    - 'By_Gender': группировка
    - 'By_Education': группировка по образованию родителей

---

## Задача 5: Продажи супермаркета

**Датасет**: Supermarket Sales  
**Источник**: Kaggle  
**Прямая ссылка**: https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales  
**Лицензия**: ODbL

**Описание**: Продажи в трех филиалах за 3 месяца (2019). Столбцы: Invoice ID, Branch, City, Customer type, Gender, Product line, Unit price, Quantity, Tax, Total, Date, Time, Payment, cogs, gross margin percentage, gross income, Rating.

### Загрузка и первичный анализ

1. Загрузите датасет
2. Выведите первые 10 строк
3. Выведите информацию о структуре
4. Выведите описательную статистику
5. Выведите список колонок
6. Проверьте размер

### Обработка пропусков и дубликатов

7. Подсчитайте пропуски
8. Вычислите процент пропусков
9. Проверьте пропуски (их не должно быть)
10. Проверьте дубликаты по `Invoice ID`
11. Проверьте полные дубликаты
12. Удалите дубликаты

### Преобразование типов данных

13. Преобразуйте `Date` в datetime
14. Преобразуйте `Time` в datetime/time
15. Объедините `Date` и `Time` в `Datetime`

### Создание новых признаков

16. Извлеките `day_of_week` из Date
17. Извлеките `month` и `month_name`
18. Извлеките `hour` из Time
19. Создайте `time_of_day`: Morning (6-12), Afternoon (12-18), Evening (18-24)
20. Создайте `revenue` = `Total`
21. Создайте `profit` = `gross income`
22. Выведите первые 10 строк

### Фильтрация и сортировка данных

23. Отфильтруйте продажи Branch A
24. Сохраните в `branch_a_sales`
25. Отфильтруйте покупки на сумму > 500
26. Используя `query()`: `Rating > 8 and Total > 300`
27. Отсортируйте по `Total` (убывание)
28. Отсортируйте по `Branch`, затем по `Total`
29. Найдите топ-20 продаж

### Группировка и агрегация

30. Сгруппируйте по `Branch` и подсчитайте продажи
31. Для каждого филиала вычислите общую выручку
32. Найдите филиал с максимальной выручкой
33. Сгруппируйте по `Product line` и вычислите:
    - Количество продаж
    - Общую выручку
    - Средний чек
    - Средний рейтинг
34. Используя `agg()`, для каждого `Branch`:
```python
{
    'Total': ['sum', 'mean', 'count'],
    'Rating': ['mean', 'std'],
    'gross income': 'sum'
}
```
35. Найдите топ-5 `Product line` по выручке

### Сводные таблицы

36. Создайте сводную: `Branch` × `Product line` с суммой Total
37. Создайте сводную: `day_of_week` × `time_of_day` с количеством продаж
38. Добавьте итоги
39. Создайте `crosstab` для `Branch` и `Payment`

### Объединение датафреймов

40. Создайте `branch_stats` со статистикой по филиалам
41. Создайте `product_stats` со статистикой по продуктам
42. Объедините основной датасет с `branch_stats`
43. Разделите по филиалам
44. Объедините обратно

### Сохранение результатов

45. Сохраните в Excel:
    - 'Sales': все продажи
    - 'By_Branch': по филиалам
    - 'By_Product': по продуктам
    - 'Daily': дневная статистика

---

Начинаю с полных формулировок задач 6-10. Разделю на два ответа.

---

## Задача 6: Анализ проката велосипедов

**Датасет**: Bike Sharing Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip  
**Лицензия**: CC BY 4.0

**Описание**: Почасовые и ежедневные данные о прокате велосипедов Capital Bikeshare (Вашингтон, США) за 2011-2012 годы с погодными условиями. Содержит hour.csv (17,379 записей) и day.csv (731 запись). Столбцы: instant (индекс записи), dteday (дата), season (сезон: 1-зима, 2-весна, 3-лето, 4-осень), yr (год: 0-2011, 1-2012), mnth (месяц 1-12), hr (час 0-23, только в hour.csv), holiday (праздник), weekday (день недели), workingday (рабочий день), weathersit (погода: 1-ясно, 2-туман, 3-легкий дождь/снег, 4-сильный дождь), temp (нормализованная температура), atemp (ощущаемая температура), hum (влажность), windspeed (скорость ветра), casual (количество незарегистрированных пользователей), registered (зарегистрированных), cnt (общее количество прокатов).

### Загрузка и первичный анализ

1. Загрузите датасет day.csv (дневные данные)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных (типы столбцов, количество записей)
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца (их не должно быть)
9. Проверьте наличие полных дубликатов в датасете
10. Проверьте наличие дубликатов по столбцу `dteday`
11. Удалите все найденные дубликаты если есть
12. Проверьте непрерывность временного ряда (все ли дни присутствуют)

### Преобразование типов данных

13. Преобразуйте столбец `dteday` в формат datetime
14. Проверьте, что преобразование выполнено корректно (выведите тип данных)
15. Преобразуйте категориальные столбцы (season, weathersit) в тип category

### Создание новых признаков

16. Денормализуйте температуру: `temp_celsius` = `temp` × 41 - 8
17. Денормализуйте ощущаемую температуру: `atemp_celsius` = `atemp` × 50 - 16
18. Денормализуйте влажность: `humidity_percent` = `hum` × 100
19. Денормализуйте скорость ветра: `windspeed_kmh` = `windspeed` × 67
20. Создайте столбец `season_name`: Winter, Spring, Summer, Fall
21. Создайте столбец `weather_category`: Clear, Cloudy, Rain, Heavy_Rain
22. Создайте столбец `temp_category`: Cold (<10°C), Mild (10-20), Warm (20-30), Hot (>30)
23. Создайте столбец `casual_ratio` = `casual` / `cnt` × 100
24. Создайте столбец `is_good_weather` (True если weathersit == 1)
25. Создайте столбец `year_season` комбинируя yr и season
26. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

27. Отфильтруйте данные только за 2012 год (yr == 1)
28. Сохраните результат в переменную `year_2012`
29. Отфильтруйте дни с отличной погодой и высоким спросом (is_good_weather и cnt > 5000)
30. Используя метод `query()`, найдите дни: `temp_celsius > 25 and weekday >= 5 and cnt > 6000`
31. Отсортируйте датасет по количеству прокатов (от большего к меньшему)
32. Отсортируйте по `season`, затем по `cnt`
33. Найдите топ-30 дней с наибольшим количеством прокатов используя `nlargest()`

### Группировка и агрегация

34. Сгруппируйте данные по `season_name` и подсчитайте среднее количество прокатов
35. Для каждого сезона вычислите среднюю долю casual пользователей
36. Найдите сезон с наибольшим средним количеством прокатов
37. Сгруппируйте по `weekday` и вычислите:
    - Среднее количество прокатов (cnt)
    - Среднее количество casual и registered
    - Среднюю температуру
    - Процент дней с хорошей погодой
38. Используя метод `agg()`, для каждого `weather_category` вычислите:
```python
{
    'cnt': ['mean', 'min', 'max', 'std'],
    'temp_celsius': ['mean', 'std'],
    'humidity_percent': 'mean',
    'casual_ratio': 'mean'
}
```
39. Найдите комбинацию погодных условий с максимальным спросом

### Сводные таблицы

40. Создайте сводную таблицу: `season_name` в строках, `weather_category` в столбцах, среднее cnt
41. Создайте сводную таблицу: `mnth` × `workingday` со средним количеством прокатов
42. Создайте сводную таблицу с casual_ratio: `season_name` × `weekday`
43. Добавьте итоговые суммы к сводной таблице (параметр `margins=True`)
44. Создайте таблицу `crosstab` для `season_name` и `workingday`

### Объединение датафреймов

45. Загрузите также hour.csv (почасовые данные)
46. Агрегируйте hour.csv до дневного уровня и сравните с day.csv
47. Создайте отдельный датафрейм `season_stats` со статистикой по сезонам
48. Создайте датафрейм `weather_impact` с анализом влияния погоды
49. Объедините основной датасет и `season_stats` используя `merge()` по столбцу `season`
50. Объедините результат с `weather_impact` по столбцу `weathersit`
51. Разделите датасет на 2011 и 2012 годы
52. Объедините эти два датасета обратно используя `pd.concat()` с параметром `ignore_index=True`

### Временной анализ

53. Установите `dteday` как индекс датафрейма
54. Вычислите скользящее среднее за 7 дней для cnt
55. Вычислите скользящее среднее за 30 дней для cnt
56. Создайте столбец с месячными средними значениями (resample)

### Сохранение результатов

57. Сохраните результаты в файл `bike_sharing_analysis.xlsx` со следующими листами:
    - 'Daily_Data': дневные данные с новыми признаками
    - 'Year_2012': данные за 2012 год
    - 'By_Season': группировка по сезонам
    - 'By_Weather': группировка по погоде
    - 'By_Day_of_Week': анализ по дням недели
    - 'Top_Days': топ-30 дней с максимальным спросом
    - 'Trends': временные тренды

---

## Задача 7: Анализ качества вина

**Датасет**: Wine Quality Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/186/wine+quality  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/186/wine+quality.zip  
**Лицензия**: CC BY 4.0

**Описание**: Физико-химические свойства португальского вина "Vinho Verde" (красное и белое) и экспертные оценки качества. Содержит winequality-red.csv (1599 записей) и winequality-white.csv (4898 записей). Столбцы: fixed acidity (фиксированная кислотность), volatile acidity (летучая кислотность), citric acid (лимонная кислота), residual sugar (остаточный сахар), chlorides (хлориды), free sulfur dioxide (свободный SO2), total sulfur dioxide (общий SO2), density (плотность), pH, sulphates (сульфаты), alcohol (алкоголь %), quality (оценка качества 0-10).

### Загрузка и первичный анализ

1. Загрузите датасет winequality-red.csv (разделитель - точка с запятой)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных (типы столбцов, количество записей)
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца (их не должно быть)
9. Проверьте наличие полных дубликатов в датасете
10. Проверьте наличие дубликатов по всем столбцам кроме quality
11. Удалите все найденные дубликаты
12. Проверьте распределение оценок quality

### Преобразование типов данных

13. Убедитесь, что все химические параметры имеют тип float
14. Преобразуйте столбец `quality` в int
15. Создайте копию датасета для дальнейшей работы

### Создание новых признаков

16. Создайте столбец `quality_category`: Poor (3-4), Average (5-6), Good (7-8), Excellent (9-10)
17. Создайте столбец `alcohol_level`: Low (<10%), Medium (10-12%), High (>12%)
18. Создайте столбец `total_acidity` = `fixed acidity` + `volatile acidity` + `citric acid`
19. Создайте столбец `sulfur_ratio` = `free sulfur dioxide` / `total sulfur dioxide`
20. Создайте столбец `sweetness_category`: Dry (<4g), Off-Dry (4-12), Sweet (>12) на основе residual sugar
21. Создайте столбец `acidity_level`: Low_pH (pH < 3.0), Medium_pH (3.0-3.4), High_pH (>3.4)
22. Создайте столбец `chlorides_category`: Low, Medium, High на основе квартилей
23. Создайте столбец `is_high_quality` (True если quality >= 7)
24. Создайте столбец `balance_score` как комбинацию нормализованных параметров
25. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

26. Отфильтруйте вина высокого качества (quality >= 7)
27. Сохраните результат в переменную `high_quality_wines`
28. Отфильтруйте вина с высоким содержанием алкоголя и низкой кислотностью
29. Используя метод `query()`, найдите вина: `alcohol > 12 and quality >= 7 and pH < 3.5`
30. Отсортируйте датасет по качеству (от большего к меньшему)
31. Отсортируйте по `alcohol`, затем по `quality`
32. Найдите топ-50 вин с наивысшим содержанием алкоголя используя `nlargest()`

### Группировка и агрегация

33. Сгруппируйте данные по `quality` и подсчитайте количество вин в каждой категории
34. Для каждой оценки качества вычислите средние значения всех химических параметров
35. Найдите оценку качества с наибольшим количеством образцов
36. Сгруппируйте по `quality_category` и вычислите:
    - Количество вин
    - Средний alcohol
    - Средний pH
    - Средняя total acidity
    - Стандартное отклонение alcohol
37. Используя метод `agg()`, для каждого `alcohol_level` вычислите:
```python
{
    'quality': ['mean', 'min', 'max', 'std'],
    'fixed acidity': ['mean', 'std'],
    'volatile acidity': ['mean', 'std'],
    'pH': 'mean',
    'sulphates': 'mean'
}
```
38. Найдите химические параметры, наиболее коррелирующие с качеством

### Сводные таблицы

39. Создайте сводную таблицу: `quality_category` в строках, `alcohol_level` в столбцах, количество вин
40. Создайте сводную таблицу со средним alcohol: `sweetness_category` × `acidity_level`
41. Создайте сводную таблицу с процентом high_quality: `alcohol_level` × `acidity_level`
42. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
43. Создайте таблицу `crosstab` для `alcohol_level` и `quality_category`

### Объединение датафреймов

44. Загрузите также winequality-white.csv
45. Добавьте столбец `wine_type` ('red' или 'white') в оба датасета
46. Объедините красные и белые вина используя `pd.concat()`
47. Создайте отдельный датафрейм `quality_benchmarks` с эталонными значениями для каждой категории
48. Создайте датафрейм `chemical_stats` со статистикой по химическим параметрам
49. Объедините основной датасет и `quality_benchmarks` используя `merge()`
50. Разделите объединенный датасет обратно по типу вина
51. Сравните статистику красных и белых вин

### Корреляционный анализ

52. Создайте корреляционную матрицу для всех числовых признаков
53. Найдите признаки с наибольшей положительной корреляцией с quality
54. Найдите признаки с наибольшей отрицательной корреляцией с quality

### Сохранение результатов

55. Сохраните результаты в файл `wine_quality_analysis.xlsx` со следующими листами:
    - 'Red_Wine': красные вина с новыми признаками
    - 'High_Quality': вина высокого качества
    - 'By_Quality': группировка по качеству
    - 'By_Alcohol': группировка по содержанию алкоголя
    - 'Chemical_Analysis': анализ химических параметров
    - 'Correlation_Matrix': корреляции
    - 'Quality_Profile': профиль качественного вина
    - 'Red_vs_White': сравнение красных и белых вин (если загружали оба)

---

## Задача 8: Анализ доходов населения (Census Income)

**Датасет**: Adult Income Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/2/adult  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/2/adult.zip  
**Лицензия**: CC BY 4.0

**Описание**: Данные переписи населения США 1994 года для предсказания, превышает ли годовой доход человека $50K. Содержит 48,842 записи (32,561 в обучающей выборке, 16,281 в тестовой). Столбцы: age (возраст), workclass (класс занятости), fnlwgt (финальный вес), education (образование), education-num (количество лет образования), marital-status (семейное положение), occupation (профессия), relationship (роль в семье), race (раса), sex (пол), capital-gain (прирост капитала), capital-loss (потери капитала), hours-per-week (часов работы в неделю), native-country (страна рождения), income (>50K или <=50K).

### Загрузка и первичный анализ

1. Загрузите датасет adult.data (нет заголовков, добавьте их вручную)
2. Названия столбцов: age, workclass, fnlwgt, education, education_num, marital_status, occupation, relationship, race, sex, capital_gain, capital_loss, hours_per_week, native_country, income
3. Выведите первые 15 строк датасета
4. Выведите информацию о структуре данных (типы столбцов, количество записей)
5. Выведите описательную статистику для всех числовых столбцов
6. Выведите список всех колонок датасета
7. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

8. Подсчитайте количество значений '?' в каждом столбце (это пропуски)
9. Замените ' ?' на NaN во всех столбцах
10. Удалите пробелы в начале/конце значений во всех строковых столбцах (используйте str.strip())
11. Вычислите процент пропусков для каждого столбца
12. Удалите строки с пропущенными значениями в workclass, occupation или native_country
13. Проверьте наличие полных дубликатов в датасете
14. Удалите все найденные дубликаты

### Преобразование типов данных

15. Преобразуйте столбец `income` в бинарный: '>50K'=1, '<=50K'=0
16. Создайте столбец `is_high_income` как булевую версию income
17. Преобразуйте категориальные столбцы в тип category
18. Убедитесь, что числовые столбцы имеют правильный тип (int или float)

### Создание новых признаков

19. Создайте столбец `age_group`: Youth (<25), Young_Adult (25-35), Adult (35-50), Middle_Age (50-65), Senior (>65)
20. Создайте столбец `education_level`: Less_HS, HS_grad, Some_College, Bachelors, Masters, Doctorate на основе education
21. Создайте столбец `hours_category`: Part_Time (<35), Full_Time (35-45), Overtime (>45)
22. Создайте столбец `workclass_simplified` группируя: Private, Self-emp, Gov (Federal/State/Local)
23. Создайте столбец `marital_simplified`: Married, Single, Divorced/Separated
24. Создайте столбец `has_capital_gain` (True если capital_gain > 0)
25. Создайте столбец `has_capital_loss` (True если capital_loss > 0)
26. Создайте столбец `net_capital` = capital_gain - capital_loss
27. Создайте столбец `is_native` (True если native_country == 'United-States')
28. Создайте столбец `income_indicators` = количество позитивных индикаторов (образование, часы работы, капитал)
29. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

30. Отфильтруйте людей с высоким доходом (income == '>50K')
31. Сохраните результат в переменную `high_earners`
32. Отфильтруйте людей с высшим образованием (Bachelors или выше)
33. Используя метод `query()`, найдите: `age > 40 and education_num >= 13 and hours_per_week > 40`
34. Отсортируйте датасет по часам работы (от большего к меньшему)
35. Отсортируйте по `education_num`, затем по `age`
36. Найдите топ-100 людей с наибольшим приростом капитала используя `nlargest()`

### Группировка и агрегация

37. Сгруппируйте данные по `education_level` и подсчитайте количество людей
38. Для каждого уровня образования вычислите процент с высоким доходом
39. Найдите уровень образования с наивысшим процентом высоких доходов
40. Сгруппируйте по `occupation` (топ-10) и вычислите:
    - Количество людей
    - Процент с высоким доходом
    - Средний возраст
    - Средние часы работы в неделю
    - Средний прирост капитала
41. Используя метод `agg()`, для каждого `age_group` вычислите:
```python
{
    'income': lambda x: (x == '>50K').mean(),  # процент высоких доходов
    'hours_per_week': ['mean', 'std'],
    'capital_gain': ['mean', 'max'],
    'education_num': 'mean'
}
```
42. Найдите топ-5 профессий по среднему доходу

### Сводные таблицы

43. Создайте сводную таблицу: `education_level` в строках, `sex` в столбцах, процент высоких доходов
44. Создайте сводную таблицу: `age_group` × `marital_simplified` со средними hours_per_week
45. Создайте сводную таблицу с количеством людей: `workclass_simplified` × `education_level`
46. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
47. Создайте таблицу `crosstab` для `sex` и `income`
48. Создайте таблицу `crosstab` с нормализацией по строкам для анализа пропорций

### Объединение датафреймов

49. Создайте отдельный датафрейм `education_stats` со статистикой по уровням образования
50. Создайте датафрейм `occupation_stats` со статистикой по профессиям
51. Создайте датафрейм `demographic_stats` с демографической статистикой
52. Объедините основной датасет и `education_stats` используя `merge()` по столбцу `education_level`
53. Объедините результат с `occupation_stats` по столбцу `occupation`
54. Разделите датасет на людей с высоким и низким доходом
55. Объедините их обратно используя `pd.concat()` с меткой источника

### Анализ неравенства

56. Вычислите процент высоких доходов по полу
57. Вычислите процент высоких доходов по расе
58. Проанализируйте разницу в часах работы между полами

### Сохранение результатов

59. Сохраните результаты в файл `census_income_analysis.xlsx` со следующими листами:
    - 'Clean_Data': очищенные данные (первые 10000 строк)
    - 'High_Earners': люди с высоким доходом
    - 'By_Education': группировка по образованию
    - 'By_Occupation': группировка по профессиям
    - 'By_Demographics': демографический анализ
    - 'Income_Gap_Analysis': анализ разрыва в доходах
    - 'Predictive_Factors': факторы предсказания дохода

---

## Задача 9: Объединение датафреймов - Успеваемость португальских студентов

**Датасет**: Student Performance Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/320/student+performance  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/320/student+performance.zip  
**Лицензия**: CC BY 4.0

**Описание**: Оценки португальских студентов по двум предметам - математике и португальскому языку. Содержит student-mat.csv (395 студентов) и student-por.csv (649 студентов). Многие студенты присутствуют в обоих датасетах. Столбцы: school (школа), sex (пол), age (возраст), address (тип адреса: U-городской/R-сельский), famsize (размер семьи), Pstatus (статус родителей), Medu (образование матери 0-4), Fedu (образование отца 0-4), Mjob (работа матери), Fjob (работа отца), reason (причина выбора школы), guardian (опекун), traveltime (время в пути), studytime (время учебы), failures (прошлые неудачи), schoolsup (дополнительная поддержка), famsup (семейная поддержка), paid (дополнительные занятия), activities (внеклассные активности), nursery (посещал детский сад), higher (хочет высшее образование), internet (доступ к интернету), romantic (романтические отношения), famrel (качество семейных отношений 1-5), freetime (свободное время 1-5), goout (выходит с друзьями 1-5), Dalc (потребление алкоголя в будни 1-5), Walc (потребление алкоголя в выходные 1-5), health (состояние здоровья 1-5), absences (пропуски), G1 (оценка 1-го периода), G2 (оценка 2-го периода), G3 (финальная оценка).

### Загрузка и первичный анализ

1. Загрузите оба датасета: student-mat.csv и student-por.csv (разделитель - точка с запятой)
2. Выведите первые 10 строк каждого датасета
3. Выведите информацию о структуре обоих датасетов
4. Проверьте размеры обоих датасетов
5. Выведите описательную статистику для числовых столбцов в обоих датасетах
6. Сравните списки столбцов (они должны быть идентичными)

### Обработка пропусков и дубликатов

7. Подсчитайте пропущенные значения в обоих датасетах
8. Вычислите процент пропусков (их не должно быть)
9. Проверьте наличие дубликатов в каждом датасете отдельно
10. Удалите дубликаты в каждом датасете если есть
11. Проверьте уникальность комбинации идентифицирующих признаков (school, sex, age, address, famsize, Pstatus, Medu, Fedu, Mjob, Fjob, guardian)

### Подготовка к объединению

12. Добавьте столбец `subject` в math датасет со значением 'Math'
13. Добавьте столбец `subject` в portuguese датасет со значением 'Portuguese'
14. Создайте уникальный идентификатор студента на основе демографических признаков
15. Найдите студентов, которые присутствуют в обоих датасетах

### Преобразование типов данных

16. Преобразуйте бинарные столбцы (schoolsup, famsup, paid, activities, nursery, higher, internet, romantic) в булевый тип
17. Преобразуйте категориальные столбцы в тип category
18. Убедитесь, что числовые столбцы имеют правильный тип

### Создание новых признаков (для каждого датасета)

19. Создайте столбец `average_grade` = (G1 + G2 + G3) / 3
20. Создайте столбец `improvement` = G3 - G1
21. Создайте столбец `passed` (True если G3 >= 10)
22. Создайте столбец `performance_category`: Poor (<10), Average (10-14), Good (14-17), Excellent (>17) на основе G3
23. Создайте столбец `parents_education` = max(Medu, Fedu)
24. Создайте столбец `total_alcohol` = Dalc + Walc
25. Создайте столбец `has_support` (True если schoolsup или famsup)
26. Создайте столбец `risk_factors` = сумма негативных факторов (failures, absences, alcohol, etc.)

### Вертикальное объединение (Concatenation)

27. Объедините оба датасета вертикально используя `pd.concat([math_df, por_df])`
28. Используйте параметр `ignore_index=True` для создания нового индекса
29. Проверьте размер объединенного датасета
30. Убедитесь, что столбец `subject` правильно указывает источник
31. Подсчитайте количество записей по каждому предмету

### Горизонтальное объединение (Merge) - Поиск общих студентов

32. Создайте список ключевых столбцов для идентификации студентов: ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'guardian']
33. Выполните inner merge между math и portuguese датасетами по ключевым столбцам
34. Сохраните результат в `both_subjects` (студенты, изучающие оба предмета)
35. Добавьте суффиксы '_math' и '_por' для различения столбцов оценок
36. Подсчитайте количество студентов в обоих предметах

### Различные типы объединений

37. Выполните left merge (сохраните всех студентов по математике)
38. Выполните right merge (сохраните всех студентов по португальскому)
39. Выполните outer merge (сохраните всех студентов из обоих датасетов)
40. Используйте параметр `indicator=True` для отслеживания источника записей
41. Проанализируйте столбец `_merge`: сколько студентов только по математике, только по португальскому, и по обоим

### Анализ объединенных данных

42. Для студентов из `both_subjects` создайте столбец `better_at`: Math, Portuguese, Equal
43. Создайте столбец `math_por_diff` = abs(average_grade_math - average_grade_por)
44. Создайте столбец `consistent_performer` (True если разница оценок < 2)
45. Вычислите корреляцию между оценками по математике и португальскому

### Группировка и агрегация объединенных данных

46. Сгруппируйте вертикально объединенный датасет по `subject` и вычислите средние оценки
47. Для `both_subjects` сгруппируйте по `school` и вычислите:
    - Среднюю оценку по математике
    - Среднюю оценку по португальскому
    - Процент студентов, лучше успевающих по математике
48. Используя метод `agg()` для `both_subjects`, вычислите:
```python
{
    'G3_math': ['mean', 'std'],
    'G3_por': ['mean', 'std'],
    'math_por_diff': 'mean'
}
```
49. Найдите факторы, влияющие на успех в обоих предметах

### Сводные таблицы

50. Создайте сводную таблицу для вертикально объединенного датасета: `subject` в строках, `performance_category` в столбцах
51. Для `both_subjects` создайте сводную таблицу: `parents_education` × `better_at` с количеством студентов
52. Создайте сводную таблицу со средними оценками: `sex` × `subject`
53. Добавьте итоговые значения (параметр `margins=True`)

### Специализированный анализ

54. Создайте датафрейм `math_only` студентов, которые есть только в математике
55. Создайте датафрейм `portuguese_only` студентов, которые есть только в португальском
56. Сравните характеристики этих трех групп (math_only, portuguese_only, both_subjects)
57. Найдите предикторы выбора предметов

### Сохранение результатов

58. Сохраните результаты в файл `student_performance_merge_analysis.xlsx` со следующими листами:
    - 'Math_Students': все студенты математики
    - 'Portuguese_Students': все студенты португальского
    - 'Both_Subjects': студенты обоих предметов
    - 'Combined_Vertical': вертикально объединенные данные (первые 500 строк)
    - 'Math_Only': только математика
    - 'Portuguese_Only': только португальский
    - 'Merge_Statistics': статистика объединений
    - 'Comparison_Analysis': сравнительный анализ
    - 'Subject_Correlation': корреляция между предметами

---

## Задача 10: Диагностика рака груди

**Датасет**: Breast Cancer Wisconsin (Diagnostic) Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/17/breast+cancer+wisconsin+diagnostic.zip  
**Лицензия**: CC BY 4.0

**Описание**: Характеристики клеточных ядер, вычисленные из оцифрованных изображений тонкоигольной аспирационной биопсии груди. Содержит 569 образцов. Столбцы: ID, diagnosis (M=malignant/злокачественная, B=benign/доброкачественная), и 30 числовых признаков (10 характеристик × 3 статистики: mean, se, worst). Характеристики: radius (радиус), texture (текстура), perimeter (периметр), area (площадь), smoothness (гладкость), compactness (компактность), concavity (вогнутость), concave points (вогнутые точки), symmetry (симметрия), fractal dimension (фрактальная размерность).

### Загрузка и первичный анализ

1. Загрузите датасет wdbc.data (нет заголовков, добавьте их вручную)
2. Первые столбцы: id, diagnosis, затем для каждой из 10 характеристик: *_mean, *_se, *_worst
3. Выведите первые 10 строк датасета
4. Выведите информацию о структуре данных (типы столбцов, количество записей)
5. Выведите описательную статистику для всех числовых столбцов
6. Выведите список всех колонок датасета
7. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

8. Подсчитайте количество пропущенных значений в каждом столбце (их не должно быть)
9. Вычислите процент пропусков для каждого столбца
10. Проверьте наличие полных дубликатов в датасете
11. Проверьте уникальность столбца `id`
12. Удалите все найденные дубликаты если есть
13. Проверьте распределение диагнозов (M vs B)

### Преобразование типов данных

14. Преобразуйте столбец `diagnosis` в бинарный: M=1 (malignant), B=0 (benign)
15. Создайте столбец `is_malignant` как булевую версию diagnosis
16. Установите `id` как индекс датафрейма
17. Убедитесь, что все числовые признаки имеют тип float

### Создание новых признаков

18. Создайте столбец `radius_category`: Small, Medium, Large на основе radius_mean квартилей
19. Создайте столбец `area_category`: Small, Medium, Large на основе area_mean квартилей
20. Создайте столбец `texture_level`: Smooth, Moderate, Rough на основе texture_mean
21. Создайте столбец `compactness_level`: Low, Medium, High на основе compactness_mean
22. Создайте столбец `symmetry_level`: Low, Medium, High на основе symmetry_mean
23. Создайте столбец `severity_score` как взвешенную сумму нормализованных worst признаков
24. Создайте столбец `mean_features_avg` = среднее всех *_mean признаков
25. Создайте столбец `worst_features_avg` = среднее всех *_worst признаков
26. Создайте столбец `variability` = среднее всех *_se признаков
27. Создайте столбец `worst_to_mean_ratio` для ключевых признаков
28. Выведите первые 10 строк с новыми столбцами

### Обработка выбросов

29. Для каждого числового признака вычислите IQR (межквартильный размах)
30. Определите границы выбросов: Q1 - 1.5×IQR и Q3 + 1.5×IQR
31. Подсчитайте количество выбросов в каждом столбце
32. Создайте столбец `has_outliers` (True если есть выбросы в любом признаке)
33. Сохраните информацию о выбросах в отдельный датафрейм

### Нормализация данных

34. Создайте копию датафрейма для нормализации
35. Примените Min-Max нормализацию ко всем числовым признакам: (x - min) / (max - min)
36. Создайте столбцы с префиксом `norm_` для нормализованных признаков
37. Примените Z-score стандартизацию: (x - mean) / std
38. Создайте столбцы с префиксом `zscore_` для стандартизованных признаков

### Фильтрация и сортировка данных

39. Отфильтруйте злокачественные опухоли (diagnosis == 1)
40. Сохраните результат в переменную `malignant_cases`
41. Отфильтруйте случаи с большим радиусом (radius_mean > 15)
42. Используя метод `query()`, найдите: `area_mean > 1000 and concavity_mean > 0.2 and diagnosis == 1`
43. Отсортируйте по severity_score (от большего к меньшему)
44. Отсортируйте по `diagnosis`, затем по `radius_mean`
45. Найдите топ-50 случаев с наибольшей площадью используя `nlargest()`

### Группировка и агрегация

46. Сгруппируйте данные по `diagnosis` и подсчитайте количество образцов
47. Для каждого типа диагноза вычислите средние значения всех числовых признаков
48. Найдите признаки с наибольшей разницей между типами
49. Сгруппируйте по `radius_category` и вычислите:
    - Количество образцов
    - Процент злокачественных
    - Средние значения ключевых признаков
    - Средний severity_score
50. Используя метод `agg()`, для каждого типа `diagnosis` вычислите:
```python
{
    'radius_mean': ['mean', 'std', 'min', 'max'],
    'area_mean': ['mean', 'std'],
    'concavity_mean': ['mean', 'std'],
    'compactness_mean': 'mean',
    'severity_score': ['mean', 'std']
}
```
51. Найдите топ-10 признаков, наиболее различающихся между типами

### Корреляционный анализ

52. Создайте корреляционную матрицу для всех числовых признаков
53. Найдите пары признаков с наивысшей корреляцией (> 0.9)
54. Найдите признаки с наивысшей корреляцией с diagnosis
55. Создайте тепловую карту корреляций (сохраните значения для визуализации)

### Сводные таблицы

56. Создайте сводную таблицу: `radius_category` в строках, `texture_level` в столбцах, процент злокачественных
57. Создайте сводную таблицу: `area_category` × `compactness_level` с количеством образцов
58. Создайте сводную таблицу со средним severity_score: `radius_category` × `symmetry_level`
59. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
60. Создайте таблицу `crosstab` для `radius_category` и `diagnosis`

### Объединение датафреймов

61. Создайте отдельный датафрейм `mean_features` только с *_mean признаками
62. Создайте датафрейм `worst_features` только с *_worst признаками
63. Создайте датафрейм `se_features` только с *_se признаками
64. Создайте датафрейм `diagnosis_stats` со статистикой по типам диагноза
65. Объедините основной датасет и `diagnosis_stats` используя `merge()` по столбцу `diagnosis`
66. Создайте датафрейм `feature_importance` с важностью каждого признака
67. Разделите датасет на доброкачественные и злокачественные образцы
68. Объедините их обратно используя `pd.concat()` с метками

### Статистический анализ

69. Вычислите статистическую значимость различий между группами (можно использовать описательную статистику)
70. Создайте профиль типичной злокачественной опухоли
71. Создайте профиль типичной доброкачественной опухоли

### Сохранение результатов

72. Сохраните результаты в файл `breast_cancer_analysis.xlsx` со следующими листами:
    - 'Clean_Data': очищенные данные с новыми признаками
    - 'Malignant': злокачественные случаи
    - 'Benign': доброкачественные случаи
    - 'Normalized_Data': нормализованные признаки (первые 300 строк)
    - 'Statistics_by_Diagnosis': статистика по диагнозам
    - 'Correlation_Matrix': корреляционная матрица
    - 'Feature_Analysis': анализ признаков
    - 'Outliers_Report': отчет о выбросах
    - 'Clinical_Profiles': клинические профили
    - 'Predictive_Features': предикторы диагноза

---


Отлично! Начинаю детально расписывать задачи с 11-й. Разделю на несколько ответов для удобства.

---

## Задача 11: Классификация цветов Ириса

**Датасет**: Iris Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/53/iris  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/53/iris.zip  
**Лицензия**: CC BY 4.0

**Описание**: Классический датасет для классификации. Содержит измерения 150 цветов ириса трёх видов (Setosa, Versicolor, Virginica). Столбцы: sepal length, sepal width, petal length, petal width, class.

### Загрузка и первичный анализ

1. Загрузите датасет iris.data (добавьте имена столбцов вручную)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных (типы столбцов, количество записей)
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Проверьте наличие полных дубликатов в датасете
10. Проверьте наличие дубликатов по всем признакам кроме класса
11. Удалите найденные дубликаты
12. Проверьте итоговое количество уникальных образцов

### Преобразование типов данных

13. Убедитесь, что все числовые столбцы имеют тип float
14. Преобразуйте столбец `class` в категориальный тип
15. Проверьте уникальные значения в столбце `class`

### Создание новых признаков

16. Создайте столбец `sepal_area` = `sepal length` × `sepal width`
17. Создайте столбец `petal_area` = `petal length` × `petal width`
18. Создайте столбец `sepal_ratio` = `sepal length` / `sepal width`
19. Создайте столбец `petal_ratio` = `petal length` / `petal width`
20. Создайте столбец `total_length` = `sepal length` + `petal length`
21. Создайте столбец `size_category`: Small, Medium, Large на основе `petal_area`
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте только цветы класса Iris-setosa
24. Сохраните результат в переменную `setosa`
25. Отфильтруйте цветы с `petal length` > 5.0
26. Используя метод `query()`, найдите цветы: `sepal_length > 6.0 and petal_width > 1.5`
27. Отсортируйте датасет по `petal length` (от меньшего к большему)
28. Отсортируйте по `class`, затем по `petal area`
29. Найдите топ-20 цветов с наибольшей `sepal_area` используя `nlargest()`

### Группировка и агрегация

30. Сгруппируйте данные по `class` и подсчитайте количество образцов
31. Для каждого класса вычислите средние значения всех числовых признаков
32. Найдите класс с наибольшим средним значением `petal length`
33. Сгруппируйте по `class` и вычислите:
    - Среднее значение всех размеров
    - Минимальное и максимальное значение `petal_area`
    - Стандартное отклонение `sepal_ratio`
34. Используя метод `agg()`, для каждого класса вычислите:
```python
{
    'sepal length': ['mean', 'min', 'max', 'std'],
    'petal length': ['mean', 'min', 'max', 'std'],
    'petal_area': ['mean', 'median']
}
```
35. Найдите класс с максимальной вариабельностью (std) по `sepal width`

### Сводные таблицы

36. Создайте сводную таблицу: `class` в строках, агрегированные средние по всем признакам
37. Создайте сводную таблицу: `size_category` × `class` с количеством образцов
38. Добавьте итоговые суммы к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `class` и `size_category`

### Объединение датафреймов

40. Создайте отдельный датафрейм `class_stats` со статистикой по каждому классу
41. Создайте датафрейм `size_stats` со статистикой по размерным категориям
42. Объедините основной датасет и `class_stats` используя `merge()` по столбцу `class` (left join)
43. Разделите датасет по классам на три отдельных датафрейма
44. Объедините эти три датасета обратно используя `pd.concat()` с параметром `ignore_index=True`

### Сохранение результатов

45. Сохраните результаты в файл `iris_analysis.xlsx` со следующими листами:
    - 'Raw_Data': исходные данные с новыми признаками
    - 'Statistics': описательная статистика по классам
    - 'Correlations': корреляционная матрица
    - 'Summary': сводные таблицы

---

## Задача 12: Оценка автомобилей

**Датасет**: Car Evaluation Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/19/car+evaluation  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/19/car+evaluation.zip  
**Лицензия**: CC BY 4.0

**Описание**: Датасет для оценки приемлемости автомобилей на основе их характеристик. Содержит 1728 записей. Столбцы: buying (цена покупки), maint (стоимость обслуживания), doors (количество дверей), persons (вместимость), lug_boot (объем багажника), safety (безопасность), class (оценка: unacc, acc, good, vgood).

### Загрузка и первичный анализ

1. Загрузите датасет car.data (добавьте имена столбцов)
2. Выведите первые 15 строк датасета
3. Выведите информацию о структуре данных
4. Выведите количество уникальных значений в каждом столбце
5. Выведите список всех колонок датасета
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Проверьте наличие полных дубликатов в датасете
10. Проверьте наличие дубликатов по всем столбцам кроме `class`
11. Удалите найденные дубликаты если есть
12. Проверьте итоговое количество записей

### Преобразование типов данных

13. Преобразуйте все столбцы в категориальный тип
14. Создайте числовое представление для столбца `buying`: vhigh=4, high=3, med=2, low=1
15. Создайте числовое представление для всех ординальных признаков

### Создание новых признаков

16. Создайте столбец `buying_score` на основе числового представления `buying`
17. Создайте столбец `maint_score` на основе числового представления `maint`
18. Создайте столбец `safety_score`: low=1, med=2, high=3
19. Создайте столбец `total_cost_score` = `buying_score` + `maint_score`
20. Создайте столбец `is_acceptable` (True если class != 'unacc')
21. Создайте столбец `quality_category` объединяя good и vgood в 'high_quality'
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте автомобили с оценкой 'good' или 'vgood'
24. Сохраните результат в переменную `quality_cars`
25. Отфильтруйте автомобили с высокой безопасностью (safety = 'high')
26. Используя метод `query()`, найдите автомобили: `safety == 'high' and persons != '2'`
27. Отсортируйте датасет по `class`, затем по `safety`
28. Отсортируйте по `total_cost_score` (от меньшего к большему)
29. Найдите топ-50 автомобилей с наивысшим `safety_score`

### Группировка и агрегация

30. Сгруппируйте данные по `class` и подсчитайте количество автомобилей
31. Для каждого класса вычислите процент автомобилей с высокой безопасностью
32. Найдите класс с наибольшим количеством автомобилей
33. Сгруппируйте по `safety` и вычислите:
    - Количество автомобилей
    - Процент приемлемых (`is_acceptable`)
    - Средний `total_cost_score`
34. Используя метод `agg()`, для каждой категории `buying` вычислите:
```python
{
    'is_acceptable': ['mean', 'count'],
    'safety_score': ['mean', 'std'],
    'total_cost_score': 'mean'
}
```
35. Найдите топ-3 комбинации `buying` и `maint` по количеству качественных автомобилей

### Сводные таблицы

36. Создайте сводную таблицу: `safety` в строках, `class` в столбцах, количество в значениях
37. Создайте сводную таблицу: `buying` × `maint` с процентом is_acceptable
38. Добавьте итоговые суммы к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `safety` и `is_acceptable`

### Объединение датафреймов

40. Создайте датафрейм `safety_stats` со статистикой по уровням безопасности
41. Создайте датафрейм `cost_stats` со статистикой по стоимостным категориям
42. Объедините основной датасет и `safety_stats` используя `merge()` по столбцу `safety`
43. Разделите датасет на приемлемые и неприемлемые автомобили
44. Объедините их обратно используя `pd.concat()`

### Сохранение результатов

45. Сохраните результаты в файл `car_evaluation_analysis.xlsx` со следующими листами:
    - 'All_Cars': все данные с новыми признаками
    - 'Quality_Cars': только good и vgood
    - 'By_Safety': группировка по безопасности
    - 'Pivot_Tables': сводные таблицы

---

## Задача 13: Банковский маркетинг

**Датасет**: Bank Marketing Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/222/bank+marketing  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/222/bank+marketing.zip  
**Лицензия**: CC BY 4.0

**Описание**: Данные о прямых маркетинговых кампаниях португальского банка (телефонные звонки). Содержит 45211 записей. Столбцы включают: age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome, y (подписался ли клиент на депозит).

### Загрузка и первичный анализ

1. Загрузите датасет bank-full.csv (разделитель - точка с запятой)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для числовых столбцов
5. Выведите список всех колонок
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Замените значения 'unknown' на NaN в категориальных столбцах
10. Проверьте наличие полных дубликатов в датасете
11. Удалите дубликаты если есть
12. Заполните пропуски в `job` модой (самым частым значением)

### Преобразование типов данных

13. Преобразуйте столбец `y` (целевая переменная) в бинарный: yes=1, no=0
14. Преобразуйте `default`, `housing`, `loan` в бинарные (yes=1, no=0)
15. Преобразуйте категориальные столбцы в тип category

### Создание новых признаков

16. Создайте столбец `age_group`: young (<30), middle (30-50), senior (>50)
17. Создайте столбец `balance_category`: negative, low (0-1000), medium (1000-5000), high (>5000)
18. Создайте столбец `contact_intensity` = `campaign` (количество контактов)
19. Создайте столбец `was_contacted_before` (True если `previous` > 0)
20. Создайте столбец `days_since_contact_category` на основе `pdays`
21. Создайте столбец `has_loans` (True если housing=1 или loan=1)
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте клиентов, которые подписались на депозит (y=1)
24. Сохраните результат в переменную `subscribers`
25. Отфильтруйте клиентов старше 60 лет с положительным балансом
26. Используя метод `query()`, найдите: `age < 30 and balance > 0 and y == 1`
27. Отсортируйте по `duration` (продолжительность звонка) по убыванию
28. Отсортируйте по `job`, затем по `age`
29. Найдите топ-100 клиентов по `duration` используя `nlargest()`

### Группировка и агрегация

30. Сгруппируйте по `job` и подсчитайте количество клиентов
31. Для каждой профессии вычислите процент подписавшихся (conversion rate)
32. Найдите профессию с наивысшим conversion rate
33. Сгруппируйте по `age_group` и вычислите:
    - Количество клиентов
    - Средний balance
    - Процент подписавшихся
    - Среднее количество контактов
34. Используя метод `agg()`, для каждого `marital` статуса вычислите:
```python
{
    'age': ['mean', 'min', 'max'],
    'balance': ['mean', 'median'],
    'duration': 'mean',
    'y': 'mean'
}
```
35. Найдите топ-5 профессий по среднему балансу

### Сводные таблицы

36. Создайте сводную таблицу: `job` в строках, `education` в столбцах, процент подписавшихся
37. Создайте сводную таблицу: `age_group` × `balance_category` с conversion rate
38. Добавьте итоговые значения (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `housing` и `y`

### Объединение датафреймов

40. Создайте датафрейм `job_stats` со статистикой по профессиям
41. Создайте датафрейм `age_stats` со статистикой по возрастным группам
42. Объедините основной датасет и `job_stats` используя `merge()` по `job`
43. Разделите датасет на подписавшихся и не подписавшихся
44. Объедините обратно с добавлением столбца-индикатора источника

### Сохранение результатов

45. Сохраните результаты в файл `bank_marketing_analysis.xlsx` со следующими листами:
    - 'Clean_Data': очищенные данные (первые 5000 строк)
    - 'Subscribers': только подписавшиеся клиенты
    - 'By_Job': анализ по профессиям
    - 'By_Age': анализ по возрастным группам
    - 'Conversion_Analysis': сводные таблицы

---

## Задача 14: Классификация грибов

**Датасет**: Mushroom Classification Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/73/mushroom  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/73/mushroom.zip  
**Лицензия**: Public Domain

**Описание**: Датасет для классификации грибов на съедобные и ядовитые. Содержит 8124 записи с 22 категориальными признаками. Столбцы включают: class (e=съедобный, p=ядовитый), cap-shape, cap-surface, cap-color, bruises, odor, gill-attachment, gill-spacing, gill-size, gill-color, stalk-shape, stalk-root, stalk-surface-above-ring, stalk-surface-below-ring, stalk-color-above-ring, stalk-color-below-ring, veil-type, veil-color, ring-number, ring-type, spore-print-color, population, habitat.

### Загрузка и первичный анализ

1. Загрузите датасет agaricus-lepiota.data (добавьте имена столбцов)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите количество уникальных значений для каждого признака
5. Выведите список всех колонок датасета
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений (символ '?') в каждом столбце
8. Замените '?' на NaN
9. Вычислите процент пропусков для каждого столбца
10. Проверьте наличие полных дубликатов в датасете
11. Удалите строки с пропущенным значением в `stalk-root`
12. Проверьте итоговое количество записей

### Преобразование типов данных

13. Преобразуйте `class` в бинарный: e=0 (edible), p=1 (poisonous)
14. Преобразуйте все признаки в категориальный тип
15. Создайте словарь для расшифровки всех кодовых обозначений

### Создание новых признаков

16. Создайте столбец `is_edible` (True если class='e')
17. Создайте столбец `has_bruises` (True если bruises='t')
18. Создайте столбец `odor_category`: pleasant, unpleasant, none (группировка запахов)
19. Создайте столбец `gill_color_group` объединяя похожие цвета
20. Создайте столбец `habitat_type`: forest, meadow, urban (группировка мест обитания)
21. Создайте столбец `ring_presence` на основе ring-number
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте только съедобные грибы
24. Сохраните результат в переменную `edible_mushrooms`
25. Отфильтруйте ядовитые грибы с неприятным запахом
26. Используя метод `query()`, найдите грибы: `habitat == 'woods' and odor == 'n'`
27. Отсортируйте по `class`, затем по `habitat`
28. Отсортируйте по `odor`, затем по `cap-color`
29. Подсчитайте количество грибов для каждой комбинации `habitat` и `class`

### Группировка и агрегация

30. Сгруппируйте по `odor` и подсчитайте количество грибов
31. Для каждого типа запаха вычислите процент ядовитых грибов
32. Найдите запах, который является индикатором ядовитости (100% ядовитых)
33. Сгруппируйте по `habitat` и вычислите:
    - Количество грибов
    - Процент съедобных
    - Количество уникальных цветов шляпки
34. Используя метод `agg()`, для каждого `cap-color` вычислите:
```python
{
    'class': lambda x: (x == 'e').mean(),  # процент съедобных
    'odor': 'nunique',
    'habitat': 'nunique'
}
```
35. Найдите топ-5 признаков, наиболее коррелирующих с классом (используйте chi-square или другие методы)

### Сводные таблицы

36. Создайте сводную таблицу: `habitat` в строках, `odor` в столбцах, процент ядовитых
37. Создайте сводную таблицу: `cap-color` × `gill-color` с распределением классов
38. Добавьте итоговые значения (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `odor` и `class`

### Объединение датафреймов

40. Создайте датафрейм `odor_stats` с детальной статистикой по запахам
41. Создайте датафрейм `habitat_stats` с детальной статистикой по местам обитания
42. Объедините основной датасет и `odor_stats` используя `merge()` по `odor`
43. Разделите датасет на съедобные и ядовитые грибы
44. Объедините обратно с добавлением индикатора

### Сохранение результатов

45. Сохраните результаты в файл `mushroom_analysis.xlsx` со следующими листами:
    - 'All_Mushrooms': все данные с новыми признаками
    - 'Edible': только съедобные
    - 'Poisonous': только ядовитые
    - 'By_Odor': анализ по запахам
    - 'Safety_Guide': сводная таблица-руководство

---

## Задача 15: Классификация животных в зоопарке

**Датасет**: Zoo Animal Classification  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/111/zoo  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/111/zoo.zip  
**Лицензия**: CC BY 4.0

**Описание**: Датасет для классификации животных на 7 типов. Содержит 101 животное с 17 бинарными признаками. Столбцы: animal name, hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs (число), tail, domestic, catsize, type (1-7: млекопитающее, птица, рептилия, рыба, амфибия, насекомое, беспозвоночное).

### Загрузка и первичный анализ

1. Загрузите датасет zoo.data (добавьте имена столбцов)
2. Выведите первые 15 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для числовых признаков
5. Выведите список всех колонок
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений
8. Вычислите процент пропусков
9. Проверьте наличие полных дубликатов (кроме имени животного)
10. Проверьте уникальность имён животных
11. Удалите дубликаты если есть
12. Проверьте итоговое количество животных

### Преобразование типов данных

13. Установите столбец `animal name` как индекс
14. Преобразуйте все бинарные признаки в булевый тип
15. Преобразуйте `type` в категориальный с названиями классов

### Создание новых признаков

16. Создайте столбец `type_name`: Mammal, Bird, Reptile, Fish, Amphibian, Insect, Invertebrate
17. Создайте столбец `mobility`: airborne, aquatic, terrestrial на основе соответствующих признаков
18. Создайте столбец `has_hair_or_feathers` (любое из двух)
19. Создайте столбец `reproduction_type`: eggs, live_birth на основе eggs и milk
20. Создайте столбец `limb_count` = legs
21. Создайте столбец `size_indicator` на основе catsize
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте только млекопитающих (type=1)
24. Сохраните результат в переменную `mammals`
25. Отфильтруйте хищников (predator=1)
26. Используя метод `query()`, найдите животных: `aquatic == True and predator == True`
27. Отсортируйте по количеству ног
28. Отсортируйте по `type`, затем по наличию яда
29. Найдите всех ядовитых животных

### Группировка и агрегация

30. Сгруппируйте по `type_name` и подсчитайте количество животных
31. Для каждого типа вычислите процент хищников
32. Найдите тип с наибольшим процентом хищников
33. Сгруппируйте по `type_name` и вычислите:
    - Количество животных
    - Процент с волосами
    - Процент с перьями
    - Среднее количество ног
    - Процент ядовитых
34. Используя метод `agg()`, для каждого типа вычислите:
```python
{
    'hair': 'sum',
    'feathers': 'sum',
    'predator': 'mean',
    'venomous': 'sum',
    'legs': 'mean'
}
```
35. Найдите признаки, которые встречаются только у одного типа животных

### Сводные таблицы

36. Создайте сводную таблицу: `type_name` в строках, сумма всех бинарных признаков
37. Создайте сводную таблицу: `mobility` × `reproduction_type` с количеством животных
38. Добавьте итоговые значения (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `type_name` и `predator`

### Объединение датафреймов

40. Создайте датафрейм `type_characteristics` с типичными признаками каждого типа
41. Создайте датафрейм `dangerous_animals` (хищники или ядовитые)
42. Объедините основной датасет и `type_characteristics` по `type_name`
43. Разделите на наземных, водных и летающих животных
44. Объедините обратно используя `pd.concat()`

### Сохранение результатов

45. Сохраните результаты в файл `zoo_analysis.xlsx` со следующими листами:
    - 'All_Animals': все данные
    - 'By_Type': группировка по типам
    - 'Mammals': только млекопитающие
    - 'Predators': хищники
    - 'Characteristics_Matrix': сводная таблица признаков

---

Продолжаю с задачами 16-20.

---

## Задача 16: Диагностика сердечных заболеваний

**Датасет**: Heart Disease Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/45/heart+disease  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/45/heart+disease.zip  
**Лицензия**: CC BY 4.0

**Описание**: Датасет содержит данные о пациентах для диагностики сердечных заболеваний. Содержит 303 записи (Cleveland database - самая используемая). Столбцы: age, sex, cp (chest pain type), trestbps (resting blood pressure), chol (serum cholesterol), fbs (fasting blood sugar), restecg (resting electrocardiographic results), thalach (maximum heart rate achieved), exang (exercise induced angina), oldpeak (ST depression), slope, ca (number of major vessels), thal, num (diagnosis: 0=no disease, 1-4=disease level).

### Загрузка и первичный анализ

1. Загрузите датасет processed.cleveland.data (добавьте имена столбцов)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений (символ '?') в каждом столбце
8. Замените '?' на NaN
9. Вычислите процент пропусков для каждого столбца
10. Заполните пропуски в числовых столбцах медианными значениями
11. Проверьте наличие полных дубликатов в датасете
12. Удалите дубликаты если есть

### Преобразование типов данных

13. Преобразуйте числовые столбцы в тип float
14. Создайте бинарный столбец `has_disease` (0 если num=0, 1 если num>0)
15. Преобразуйте категориальные признаки (sex, cp, fbs, restecg, exang, slope, thal) в тип category

### Создание новых признаков

16. Создайте столбец `age_group`: young (<40), middle (40-55), senior (>55)
17. Создайте столбец `chol_level`: normal (<200), borderline (200-239), high (>=240)
18. Создайте столбец `bp_category`: normal (<120), elevated (120-129), high (>=130)
19. Создайте столбец `max_hr_category`: low (<100), normal (100-150), high (>150)
20. Создайте столбец `risk_factors` = количество факторов риска (высокий холестерин, высокое давление, и т.д.)
21. Создайте столбец `chest_pain_severity` на основе типа боли в груди (cp)
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте пациентов с сердечными заболеваниями (has_disease=1)
24. Сохраните результат в переменную `diseased_patients`
25. Отфильтруйте мужчин старше 50 лет с высоким холестерином
26. Используя метод `query()`, найдите пациентов: `age > 60 and chol > 250 and has_disease == 1`
27. Отсортируйте по уровню холестерина (от большего к меньшему)
28. Отсортируйте по `age`, затем по `chol`
29. Найдите топ-20 пациентов с наивысшим уровнем холестерина используя `nlargest()`

### Группировка и агрегация

30. Сгруппируйте данные по `sex` и подсчитайте количество пациентов
31. Для каждого пола вычислите процент пациентов с заболеванием
32. Найдите пол с более высокой заболеваемостью
33. Сгруппируйте по `age_group` и вычислите:
    - Количество пациентов
    - Процент с заболеванием
    - Средний уровень холестерина
    - Среднее давление
    - Средний максимальный пульс
34. Используя метод `agg()`, для каждого `chest_pain_severity` вычислите:
```python
{
    'age': ['mean', 'min', 'max'],
    'chol': ['mean', 'std'],
    'trestbps': 'mean',
    'has_disease': 'mean'
}
```
35. Найдите топ-3 возрастные группы по риску заболевания

### Сводные таблицы

36. Создайте сводную таблицу: `age_group` в строках, `sex` в столбцах, процент заболевших
37. Создайте сводную таблицу: `chol_level` × `bp_category` со средним risk_factors
38. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `sex` и `has_disease`

### Объединение датафреймов

40. Создайте отдельный датафрейм `high_risk` с пациентами высокого риска
41. Создайте датафрейм `age_statistics` со статистикой по возрастным группам
42. Объедините основной датасет и `age_statistics` используя `merge()` по столбцу `age_group` (left join)
43. Разделите датасет на больных и здоровых пациентов
44. Объедините их обратно используя `pd.concat()` с параметром `ignore_index=True`

### Сохранение результатов

45. Сохраните результаты в файл `heart_disease_analysis.xlsx` со следующими листами:
    - 'All_Patients': все данные с новыми признаками
    - 'Diseased': пациенты с заболеванием
    - 'High_Risk': пациенты высокого риска
    - 'By_Age_Sex': группировка по возрасту и полу
    - 'Risk_Analysis': анализ факторов риска

---

## Задача 17: Диагностика диабета

**Датасет**: Pima Indians Diabetes Dataset  
**Источник**: Kaggle  
**Прямая ссылка**: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database  
**Альтернатива**: https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv  
**Лицензия**: CC0 Public Domain

**Описание**: Датасет содержит медицинские данные женщин индейцев племени Пима для предсказания диабета. 768 записей. Столбцы: Pregnancies (количество беременностей), Glucose (уровень глюкозы), BloodPressure (артериальное давление), SkinThickness (толщина кожи), Insulin (инсулин), BMI (индекс массы тела), DiabetesPedigreeFunction (наследственность), Age (возраст), Outcome (1=диабет, 0=нет).

### Загрузка и первичный анализ

1. Загрузите датасет diabetes.csv
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество нулевых значений в каждом столбце (0 часто означает пропуск)
8. Замените нулевые значения в Glucose, BloodPressure, SkinThickness, Insulin, BMI на NaN
9. Вычислите процент пропусков для каждого столбца
10. Заполните пропуски медианными значениями по группам (Outcome)
11. Проверьте наличие полных дубликатов в датасете
12. Удалите дубликаты если есть

### Преобразование типов данных

13. Преобразуйте столбец `Outcome` в булевый тип
14. Убедитесь, что все остальные столбцы имеют тип float
15. Создайте копию датасета для дальнейшей работы

### Создание новых признаков

16. Создайте столбец `age_group`: young (<30), middle (30-50), senior (>50)
17. Создайте столбец `bmi_category`: underweight (<18.5), normal (18.5-25), overweight (25-30), obese (>30)
18. Создайте столбец `glucose_level`: normal (<140), prediabetes (140-199), diabetes (>=200)
19. Создайте столбец `bp_category`: low (<80), normal (80-89), high (>=90)
20. Создайте столбец `pregnancy_category`: none (0), few (1-3), many (>3)
21. Создайте столбец `risk_score` как сумму нормализованных факторов риска
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте пациенток с диабетом (Outcome=1)
24. Сохраните результат в переменную `diabetic_patients`
25. Отфильтруйте молодых женщин (возраст < 30) с высоким BMI
26. Используя метод `query()`, найдите: `Glucose > 140 and BMI > 30 and Outcome == 1`
27. Отсортируйте по уровню глюкозы (от большего к меньшему)
28. Отсортируйте по `Age`, затем по `Glucose`
29. Найдите топ-20 пациенток с наивысшим risk_score

### Группировка и агрегация

30. Сгруппируйте данные по `age_group` и подсчитайте количество пациенток
31. Для каждой возрастной группы вычислите процент с диабетом
32. Найдите возрастную группу с наивысшей заболеваемостью
33. Сгруппируйте по `bmi_category` и вычислите:
    - Количество пациенток
    - Процент с диабетом
    - Средний уровень глюкозы
    - Средний инсулин
    - Среднее количество беременностей
34. Используя метод `agg()`, для каждого `glucose_level` вычислите:
```python
{
    'Outcome': 'mean',
    'BMI': ['mean', 'std'],
    'Age': ['mean', 'min', 'max'],
    'Insulin': 'mean',
    'DiabetesPedigreeFunction': 'mean'
}
```
35. Найдите топ-5 комбинаций факторов с максимальным риском диабета

### Сводные таблицы

36. Создайте сводную таблицу: `age_group` в строках, `bmi_category` в столбцах, процент с диабетом
37. Создайте сводную таблицу: `glucose_level` × `pregnancy_category` со средним BMI
38. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `bmi_category` и `Outcome`

### Объединение датафреймов

40. Создайте отдельный датафрейм `high_risk_profile` с профилями высокого риска
41. Создайте датафрейм `age_bmi_stats` со статистикой по возрасту и BMI
42. Объедините основной датасет и `age_bmi_stats` используя `merge()`
43. Разделите датасет на с диабетом и без диабета
44. Объедините их обратно используя `pd.concat()`

### Сохранение результатов

45. Сохраните результаты в файл `diabetes_analysis.xlsx` со следующими листами:
    - 'Clean_Data': очищенные данные
    - 'Diabetic': пациентки с диабетом
    - 'High_Risk': группы высокого риска
    - 'By_Age_BMI': анализ по возрасту и BMI
    - 'Risk_Factors': анализ факторов риска

---

## Задача 18: Анализ футболистов FIFA

**Датасет**: FIFA 22 Complete Player Dataset  
**Источник**: Kaggle  
**Прямая ссылка**: https://www.kaggle.com/datasets/stefanoleone992/fifa-22-complete-player-dataset  
**Лицензия**: CC0 Public Domain

**Описание**: Полный датасет игроков FIFA 22 с более чем 19000 футболистов. Содержит 100+ признаков включая: short_name, age, height_cm, weight_kg, overall, potential, value_eur, wage_eur, preferred_foot, international_reputation, weak_foot, skill_moves, work_rate, body_type, position, club_name, league_name, nationality и множество атрибутов навыков.

### Загрузка и первичный анализ

1. Загрузите датасет players_22.csv
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для числовых столбцов
5. Выведите список всех колонок и выберите 20 наиболее важных для анализа
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в ключевых столбцах
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в числовых признаках навыков медианными значениями по позициям
10. Удалите столбцы с более чем 50% пропусков
11. Проверьте наличие дубликатов по имени игрока
12. Оставьте только уникальных игроков

### Преобразование типов данных

13. Преобразуйте `value_eur` и `wage_eur` в числовой формат (если нужно)
14. Преобразуйте `height_cm` и `weight_kg` в float
15. Создайте категориальный тип для `preferred_foot`, `work_rate`, `body_type`

### Создание новых признаков

16. Создайте столбец `age_category`: young (<23), prime (23-30), experienced (>30)
17. Создайте столбец `value_category`: budget (<1M), medium (1-10M), expensive (10-50M), superstar (>50M)
18. Создайте столбец `overall_category`: average (<70), good (70-80), great (80-85), world_class (>85)
19. Создайте столбец `potential_gap` = `potential` - `overall`
20. Создайте столбец `bmi` = `weight_kg` / (`height_cm`/100)²
21. Создайте столбец `position_group`: GK, Defender, Midfielder, Forward на основе position
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте игроков мирового класса (overall > 85)
24. Сохраните результат в переменную `world_class_players`
25. Отфильтруйте молодых талантливых игроков (age < 23 и potential > 80)
26. Используя метод `query()`, найдите: `age < 25 and overall > 80 and value_eur > 50000000`
27. Отсортируйте по общему рейтингу (от большего к меньшему)
28. Отсортируйте по `value_eur`, затем по `overall`
29. Найдите топ-50 самых дорогих игроков используя `nlargest()`

### Группировка и агрегация

30. Сгруппируйте данные по `position_group` и подсчитайте количество игроков
31. Для каждой позиции вычислите средний overall и средний wage
32. Найдите позицию с самыми высокооплачиваемыми игроками
33. Сгруппируйте по `league_name` (топ-5 лиг) и вычислите:
    - Количество игроков
    - Средний overall
    - Средняя стоимость
    - Средняя зарплата
    - Процент игроков мирового класса
34. Используя метод `agg()`, для каждой `nationality` (топ-10 стран) вычислите:
```python
{
    'overall': ['mean', 'max'],
    'potential': 'mean',
    'value_eur': ['mean', 'max'],
    'age': 'mean'
}
```
35. Найдите топ-10 клубов по средней стоимости состава

### Сводные таблицы

36. Создайте сводную таблицу: `age_category` в строках, `position_group` в столбцах, средний overall
37. Создайте сводную таблицу: `preferred_foot` × `position_group` с количеством игроков
38. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `overall_category` и `age_category`

### Объединение датафреймов

40. Создайте отдельный датафрейм `league_stats` со статистикой по топ-5 лигам
41. Создайте датафрейм `young_talents` с перспективными молодыми игроками
42. Объедините основной датасет и `league_stats` используя `merge()` по `league_name`
43. Разделите датасет по позициям (4 датафрейма)
44. Объедините обратно используя `pd.concat()` с добавлением метки позиционной группы

### Сохранение результатов

45. Сохраните результаты в файл `fifa_analysis.xlsx` со следующими листами:
    - 'Top_Players': топ-1000 игроков
    - 'World_Class': игроки мирового класса
    - 'Young_Talents': молодые таланты
    - 'By_League': анализ по лигам
    - 'By_Position': анализ по позициям
    - 'Market_Value': анализ рыночной стоимости

---

## Задача 19: Анализ данных COVID-19

**Датасет**: COVID-19 Dataset  
**Источник**: Johns Hopkins University (через GitHub)  
**Прямая ссылка**: https://github.com/datasets/covid-19  
**Альтернатива**: https://raw.githubusercontent.com/datasets/covid-19/master/data/time-series-19-covid-combined.csv  
**Лицензия**: Public Domain

**Описание**: Временные ряды данных COVID-19 по странам и регионам. Столбцы: Date, Country/Region, Province/State, Confirmed (подтвержденные случаи), Recovered (выздоровевшие), Deaths (смерти).

### Загрузка и первичный анализ

1. Загрузите датасет time-series-19-covid-combined.csv
2. Выведите первые 20 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета и временной период

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в Recovered и Deaths нулями (если нет данных)
10. Проверьте наличие дубликатов по комбинации Date, Country/Region, Province/State
11. Удалите дубликаты если есть
12. Проверьте целостность временных рядов

### Преобразование типов данных

13. Преобразуйте столбец `Date` в формат datetime
14. Установите `Date` как индекс датафрейма
15. Убедитесь, что числовые столбцы имеют тип int или float

### Создание новых признаков

16. Создайте столбец `Active` = `Confirmed` - `Recovered` - `Deaths`
17. Создайте столбец `Mortality_Rate` = (`Deaths` / `Confirmed`) × 100
18. Создайте столбец `Recovery_Rate` = (`Recovered` / `Confirmed`) × 100
19. Извлеките `Year`, `Month`, `Week` из столбца `Date`
20. Создайте столбец `Daily_New_Cases` (разница между днями для каждой страны)
21. Создайте столбец `7day_avg_cases` (скользящее среднее за 7 дней)
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте данные только за 2021 год
24. Сохраните результат в переменную `covid_2021`
25. Отфильтруйте топ-20 стран по общему количеству подтвержденных случаев
26. Используя метод `query()`, найдите: `Confirmed > 1000000 and Mortality_Rate > 2`
27. Отсортируйте по количеству смертей (от большего к меньшему)
28. Отсортируйте по `Country/Region`, затем по `Date`
29. Найдите дни с максимальным приростом случаев по всему миру

### Группировка и агрегация

30. Сгруппируйте по `Country/Region` и найдите итоговые Confirmed, Recovered, Deaths
31. Для каждой страны вычислите средний Mortality_Rate и Recovery_Rate
32. Найдите страну с наивысшим общим количеством случаев
33. Сгруппируйте по месяцам (для всего мира) и вычислите:
    - Сумму новых случаев
    - Сумму новых смертей
    - Среднюю смертность
    - Максимальный дневной прирост
34. Используя метод `agg()`, для топ-10 стран вычислите:
```python
{
    'Confirmed': ['max', 'sum'],
    'Deaths': ['max', 'sum'],
    'Mortality_Rate': 'mean',
    'Daily_New_Cases': 'mean'
}
```
35. Найдите топ-10 стран по темпу роста заболеваемости

### Сводные таблицы

36. Создайте сводную таблицу: `Country/Region` (топ-20) в строках, `Month` в столбцах, сумма новых случаев
37. Создайте сводную таблицу со средней смертностью: `Year` × `Month`
38. Добавьте итоговые значения (параметр `margins=True`)
39. Создайте таблицу сравнения волн пандемии по месяцам

### Объединение датафреймов

40. Создайте датафрейм `country_totals` с итоговыми показателями по странам
41. Создайте датафрейм `monthly_global` с глобальной статистикой по месяцам
42. Объедините данные за разные годы используя `merge()`
43. Разделите датасет на первую, вторую и третью волны пандемии
44. Объедините волны обратно для сравнительного анализа

### Сохранение результатов

45. Сохраните результаты в файл `covid_analysis.xlsx` со следующими листами:
    - 'Global_Summary': мировая статистика
    - 'Top_20_Countries': топ-20 стран
    - 'Monthly_Trends': помесячные тренды
    - 'Mortality_Analysis': анализ смертности
    - 'Wave_Comparison': сравнение волн

---

## Задача 20: История Олимпийских игр

**Датасет**: 120 Years of Olympic History: Athletes and Results  
**Источник**: Kaggle  
**Прямая ссылка**: https://www.kaggle.com/datasets/heesoo37/120-years-of-olympic-history-athletes-and-results  
**Лицензия**: CC0 Public Domain

**Описание**: Исторические данные об олимпийских атлетах и результатах с 1896 по 2016 год. Содержит 271,116 записей. Столбцы: ID, Name, Sex, Age, Height, Weight, Team, NOC (National Olympic Committee code), Games, Year, Season, City, Sport, Event, Medal.

### Загрузка и первичный анализ

1. Загрузите датасет athlete_events.csv
2. Выведите первые 15 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета и временной охват

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в Age, Height, Weight медианными значениями по Sport и Sex
10. Проверьте наличие дубликатов по комбинации Name, Year, Event
11. Удалите полные дубликаты
12. Проверьте корректность данных (Year, Age в разумных пределах)

### Преобразование типов данных

13. Преобразуйте `Year` в int
14. Преобразуйте `Season`, `Sex`, `Sport`, `Event` в категориальный тип
15. Создайте бинарный столбец `Won_Medal` (1 если Medal не NaN)

### Создание новых признаков

16. Создайте столбец `Age_Group`: teen (<20), young (20-25), prime (25-30), veteran (>30)
17. Создайте столбец `Height_Category`: short, medium, tall на основе квартилей
18. Создайте столбец `BMI` = `Weight` / (`Height`/100)²
19. Создайте столбец `Era`: early (1896-1936), middle (1948-1988), modern (1992-2016)
20. Создайте столбец `Medal_Type`: Gold, Silver, Bronze, None
21. Создайте столбец `Decade` из Year
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте только летние Олимпийские игры (Season='Summer')
24. Сохраните результат в переменную `summer_olympics`
25. Отфильтруйте золотых медалистов (Medal='Gold')
26. Используя метод `query()`, найдите атлетов: `Age < 18 and Won_Medal == 1`
27. Отсортируйте по количеству медалей по странам
28. Отсортируйте по `Year`, затем по `Sport`
29. Найдите топ-100 атлетов по количеству участий в Олимпиадах

### Группировка и агрегация

30. Сгруппируйте данные по `Sport` и подсчитайте количество участников
31. Для каждого вида спорта вычислите процент медалистов
32. Найдите вид спорта с наибольшим количеством участников
33. Сгруппируйте по `NOC` (топ-20 стран) и вычислите:
    - Общее количество участников
    - Количество медалей каждого типа
    - Средний возраст участников
    - Средний рост и вес
    - Процент женщин
34. Используя метод `agg()`, для каждого `Decade` вычислите:
```python
{
    'Name': 'nunique',  # уникальных атлетов
    'Won_Medal': 'mean',  # процент медалистов
    'Age': ['mean', 'std'],
    'Height': 'mean',
    'Weight': 'mean'
}
```
35. Найдите топ-10 спортсменов по количеству медалей за всю историю

### Сводные таблицы

36. Создайте сводную таблицу: `Era` в строках, `Season` в столбцах, количество участников
37. Создайте сводную таблицу: `Sport` (топ-10) × `Medal_Type` с количеством медалей
38. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `Sex` и `Medal_Type`

### Объединение датафреймов

40. Создайте отдельный датафрейм `country_medals` с медальным зачетом по странам
41. Создайте датафрейм `sport_statistics` со статистикой по видам спорта
42. Объедините основной датасет и `country_medals` используя `merge()` по `NOC`
43. Разделите на летние и зимние Олимпиады
44. Объедините обратно для сравнительного анализа

### Сохранение результатов

45. Сохраните результаты в файл `olympics_analysis.xlsx` со следующими листами:
    - 'Medal_Winners': все медалисты
    - 'Country_Rankings': медальный зачет стран
    - 'Sport_Analysis': анализ по видам спорта
    - 'Era_Comparison': сравнение эпох
    - 'Athletes_Stats': статистика атлетов
    - 'Historical_Trends': исторические тренды

---

Продолжаю с задачами 21-25, включая юридические датасеты.

---

## Задача 21: ЮРИДИЧЕСКИЙ - Анализ федеральных судебных дел США

**Датасет**: Federal Court Cases Integrated Database  
**Источник**: Federal Judicial Center / Data.gov  
**Прямая ссылка**: https://catalog.data.gov/dataset/federal-court-cases-integrated-database-series-34e8a  
**Альтернатива**: https://www.fjc.gov/research/idb (выберите Civil или Criminal)  
**Лицензия**: US Public Domain

**Описание**: Официальная база данных федеральных судебных дел США. Содержит информацию о гражданских и уголовных делах в федеральных судах. Столбцы включают (для гражданских дел): FILEDATE (дата подачи), TERMDATE (дата завершения), CIRCUIT (судебный округ), DISTRICT (район), ORIGIN (происхождение дела), JURISDICTION (юрисдикция), NOJ (природа дела), NOS (природа иска), DEMANDED (требуемая сумма), JUDGMENT (решение суда), PROCEDUR (процедура), DISP (способ разрешения), и другие.

### Загрузка и первичный анализ

1. Загрузите датасет civil.csv (гражданские дела за последний доступный год)
2. Выведите первые 20 строк датасета
3. Выведите информацию о структуре данных (типы столбцов, количество записей)
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в JUDGMENT значением 0 (если решение не указано)
10. Проверьте наличие полных дубликатов в датасете
11. Проверьте наличие дубликатов по ключевым идентификаторам
12. Удалите найденные дубликаты

### Преобразование типов данных

13. Преобразуйте столбец `FILEDATE` в формат datetime
14. Преобразуйте столбец `TERMDATE` в формат datetime
15. Проверьте, что преобразование выполнено корректно

### Создание новых признаков

16. Создайте столбец `case_duration_days` = `TERMDATE` - `FILEDATE`
17. Создайте столбец `filing_year` из столбца `FILEDATE`
18. Создайте столбец `filing_month` и `filing_quarter`
19. Создайте столбец `case_category` группируя NOJ (природу дела)
20. Создайте столбец `high_value_case` (True если DEMANDED > 1000000)
21. Создайте столбец `duration_category`: fast (<180 дней), normal (180-365), slow (365-730), very_slow (>730)
22. Создайте столбец `plaintiff_won` на основе DISP (способа разрешения)
23. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

24. Отфильтруйте дела, завершенные в течение года (case_duration_days < 365)
25. Сохраните результат в переменную `fast_cases`
26. Отфильтруйте высокостоимостные дела (high_value_case == True)
27. Используя метод `query()`, найдите дела: `case_duration_days > 730 and DEMANDED > 100000`
28. Отсортируйте датасет по длительности дела (от большего к меньшему)
29. Отсортируйте по `CIRCUIT`, затем по `FILEDATE`
30. Найдите топ-50 дел с наибольшей требуемой суммой используя `nlargest()`

### Группировка и агрегация

31. Сгруппируйте данные по `CIRCUIT` и подсчитайте количество дел
32. Для каждого округа вычислите среднюю длительность рассмотрения дела
33. Найдите округ с наибольшим количеством дел
34. Сгруппируйте по `case_category` и вычислите:
    - Количество дел
    - Среднюю длительность
    - Медианную требуемую сумму
    - Процент дел, выигранных истцом
    - Среднее решение суда
35. Используя метод `agg()`, для каждого `DISTRICT` (топ-10) вычислите:
```python
{
    'case_duration_days': ['mean', 'median', 'min', 'max'],
    'DEMANDED': ['mean', 'sum', 'count'],
    'JUDGMENT': ['mean', 'sum'],
    'plaintiff_won': 'mean'
}
```
36. Найдите топ-10 категорий дел по средней требуемой сумме

### Сводные таблицы

37. Создайте сводную таблицу: `filing_year` в строках, `case_category` (топ-5) в столбцах, количество дел
38. Создайте сводную таблицу со средней длительностью: `CIRCUIT` × `duration_category`
39. Создайте сводную таблицу с суммой JUDGMENT: `case_category` × `filing_year`
40. Добавьте итоговые суммы к сводной таблице (параметр `margins=True`)
41. Создайте таблицу `crosstab` для `CIRCUIT` и `duration_category`

### Объединение датафреймов

42. Создайте отдельный датафрейм `circuit_stats` со статистикой по округам
43. Создайте датафрейм `category_stats` со статистикой по категориям дел
44. Объедините основной датасет и `circuit_stats` используя `merge()` по `CIRCUIT` (left join)
45. Объедините результат с `category_stats` по столбцу `case_category`
46. Разделите датасет на быстрые (< 365 дней) и медленные дела (>= 365 дней)
47. Объедините эти два датасета обратно используя `pd.concat()` с параметром `ignore_index=True`

### Сохранение результатов

48. Сохраните результаты в файл `federal_courts_analysis.xlsx` со следующими листами:
    - 'All_Cases': все дела с новыми признаками (первые 10000 строк)
    - 'High_Value': высокостоимостные дела
    - 'By_Circuit': группировка по округам
    - 'By_Category': группировка по категориям
    - 'Duration_Analysis': анализ длительности
    - 'Financial_Analysis': финансовый анализ

---

## Задача 22: ЮРИДИЧЕСКИЙ - Анализ рецидивизма и предсказание повторных преступлений

**Датасет**: COMPAS Recidivism Data (ProPublica)  
**Источник**: ProPublica / Kaggle  
**Прямая ссылка**: https://www.kaggle.com/datasets/danofer/compass  
**Альтернатива**: https://github.com/propublica/compas-analysis  
**Лицензия**: Creative Commons

**Описание**: Знаменитый датасет ProPublica о системе оценки риска рецидивизма COMPAS. Содержит данные о ~7000 обвиняемых в округе Broward, Флорида (2013-2014). Столбцы включают: age, sex, race, juv_fel_count (количество тяжких преступлений в несовершеннолетнем возрасте), juv_misd_count, juv_other_count, priors_count (количество предыдущих судимостей), c_charge_degree (степень обвинения), c_charge_desc (описание обвинения), decile_score (оценка риска 1-10), score_text (Low/Medium/High), is_recid (совершил ли повторное преступление), r_charge_degree, two_year_recid (рецидивизм в течение 2 лет).

### Загрузка и первичный анализ

1. Загрузите датасет compas-scores-two-years.csv
2. Выведите первые 15 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Удалите строки с пропусками в критических столбцах (decile_score, two_year_recid)
10. Проверьте наличие полных дубликатов в датасете
11. Проверьте наличие дубликатов по id или name
12. Удалите все найденные дубликаты

### Преобразование типов данных

13. Преобразуйте столбцы с датами (c_jail_in, c_jail_out, etc.) в формат datetime
14. Преобразуйте `is_recid` и `two_year_recid` в булевый тип
15. Преобразуйте категориальные столбцы (sex, race, score_text) в category

### Создание новых признаков

16. Создайте столбец `age_group`: young (<25), adult (25-40), middle (40-55), senior (>55)
17. Создайте столбец `total_juvenile_crimes` = sum всех juvenile counts
18. Создайте столбец `criminal_history_severity`: none (0), low (1-2), medium (3-5), high (>5) на основе priors_count
19. Создайте столбец `risk_category` из score_text: Low=1, Medium=2, High=3
20. Создайте столбец `prediction_correct` (True если score_text совпадает с фактом two_year_recid)
21. Создайте столбец `days_in_jail` из дат заключения
22. Создайте столбец `charge_severity`: felony или misdemeanor на основе c_charge_degree
23. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

24. Отфильтруйте лиц с высоким риском рецидивизма (score_text='High')
25. Сохраните результат в переменную `high_risk_defendants`
26. Отфильтруйте молодых людей с предыдущими судимостями
27. Используя метод `query()`, найдите: `age < 25 and priors_count > 3 and two_year_recid == True`
28. Отсортируйте датасет по оценке риска (decile_score) от большего к меньшему
29. Отсортируйте по `race`, затем по `decile_score`
30. Найдите топ-100 лиц с наибольшим количеством предыдущих судимостей

### Группировка и агрегация

31. Сгруппируйте данные по `race` и подсчитайте количество обвиняемых
32. Для каждой расовой группы вычислите процент рецидивизма (two_year_recid)
33. Для каждой расовой группы вычислите средний decile_score
34. Сгруппируйте по `score_text` и вычислите:
    - Количество обвиняемых
    - Фактический процент рецидивизма
    - Средний возраст
    - Среднее количество предыдущих судимостей
    - Точность предсказания (prediction_correct)
35. Используя метод `agg()`, для каждой группы `age_group` × `race` вычислите:
```python
{
    'decile_score': ['mean', 'std'],
    'two_year_recid': 'mean',
    'priors_count': 'mean',
    'prediction_correct': 'mean'
}
```
36. Найдите демографические группы с наибольшим расхождением между предсказанием и реальностью

### Сводные таблицы

37. Создайте сводную таблицу: `race` в строках, `score_text` в столбцах, фактический процент рецидивизма
38. Создайте сводную таблицу: `age_group` × `criminal_history_severity` со средним decile_score
39. Создайте сводную таблицу точности предсказаний: `race` × `score_text` с процентом prediction_correct
40. Добавьте итоговые суммы к сводной таблице (параметр `margins=True`)
41. Создайте таблицу `crosstab` для `race` и `two_year_recid`

### Объединение датафреймов

42. Создайте отдельный датафрейм `race_statistics` с детальной статистикой по расовым группам
43. Создайте датафрейм `risk_accuracy` с анализом точности для каждого уровня риска
44. Объедините основной датасет и `race_statistics` используя `merge()` по столбцу `race`
45. Создайте датафреймы для тех, кто совершил рецидив и не совершил
46. Объедините их обратно используя `pd.concat()` с добавлением метки
47. Создайте итоговый датафрейм с анализом справедливости системы

### Сохранение результатов

48. Сохраните результаты в файл `recidivism_analysis.xlsx` со следующими листами:
    - 'Clean_Data': очищенные данные (первые 5000 строк)
    - 'High_Risk': обвиняемые высокого риска
    - 'Recidivists': те, кто совершил повторное преступление
    - 'By_Race': анализ по расовым группам
    - 'Accuracy_Analysis': анализ точности предсказаний
    - 'Fairness_Report': отчет о справедливости системы

---

## Задача 23: Оценка стоимости недвижимости

**Датасет**: Real Estate Valuation Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/477/real+estate+valuation+data+set.zip  
**Лицензия**: CC BY 4.0

**Описание**: Данные о ценах на недвижимость в районе Синьбэй, Тайвань. Содержит 414 записей. Столбцы: X1=transaction date (дата сделки), X2=house age (возраст дома), X3=distance to the nearest MRT station (расстояние до метро), X4=number of convenience stores (количество магазинов), X5=latitude (широта), X6=longitude (долгота), Y=house price of unit area (цена за единицу площади).

### Загрузка и первичный анализ

1. Загрузите датасет Real estate valuation data set.xlsx (используйте pd.read_excel)
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Проверьте наличие выбросов в данных (используя метод IQR)
10. Проверьте наличие полных дубликатов в датасете
11. Удалите дубликаты если есть
12. Проверьте корректность географических координат

### Преобразование типов данных

13. Переименуйте столбцы на понятные имена (transaction_date, house_age, distance_to_mrt, convenience_stores, latitude, longitude, price_per_unit)
14. Преобразуйте все числовые столбцы в float
15. Создайте копию датасета для дальнейшей работы

### Создание новых признаков

16. Создайте столбец `age_category`: new (<5 лет), recent (5-15), old (15-30), very_old (>30)
17. Создайте столбец `distance_category`: very_close (<500м), close (500-1000), medium (1000-2000), far (>2000)
18. Создайте столбец `stores_category`: few (0-2), moderate (3-5), many (>5)
19. Создайте столбец `price_category`: budget (<30), medium (30-45), premium (45-60), luxury (>60)
20. Создайте столбец `location_score` = функция от latitude, longitude (например, расстояние от центра)
21. Создайте столбец `accessibility_score` на основе distance_to_mrt и convenience_stores
22. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

23. Отфильтруйте премиальную недвижимость (price_per_unit > 45)
24. Сохраните результат в переменную `premium_properties`
25. Отфильтруйте новые дома рядом с метро (age < 5, distance_to_mrt < 500)
26. Используя метод `query()`, найдите: `house_age < 10 and distance_to_mrt < 1000 and convenience_stores > 5`
27. Отсортируйте по цене (от большей к меньшей)
28. Отсортируйте по `distance_to_mrt`, затем по `house_age`
29. Найдите топ-50 самых дорогих объектов используя `nlargest()`

### Группировка и агрегация

30. Сгруппируйте данные по `age_category` и подсчитайте количество объектов
31. Для каждой возрастной категории вычислите среднюю цену
32. Найдите возрастную категорию с наивысшей средней ценой
33. Сгруппируйте по `distance_category` и вычислите:
    - Количество объектов
    - Среднюю цену
    - Медианную цену
    - Стандартное отклонение цены
    - Средний возраст дома
34. Используя метод `agg()`, для каждой `stores_category` вычислите:
```python
{
    'price_per_unit': ['mean', 'median', 'min', 'max', 'std'],
    'house_age': 'mean',
    'distance_to_mrt': 'mean',
    'accessibility_score': 'mean'
}
```
35. Найдите комбинацию факторов с максимальной ценой

### Сводные таблицы

36. Создайте сводную таблицу: `age_category` в строках, `distance_category` в столбцах, средняя цена
37. Создайте сводную таблицу: `stores_category` × `age_category` с количеством объектов
38. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
39. Создайте таблицу `crosstab` для `age_category` и `price_category`

### Объединение датафреймов

40. Создайте отдельный датафрейм `location_stats` со статистикой по районам (на основе координат)
41. Создайте датафрейм `price_factors` с анализом влияния каждого фактора на цену
42. Объедините основной датасет и статистику по районам
43. Разделите датасет на бюджетную, среднюю и премиум недвижимость
44. Объедините обратно используя `pd.concat()`

### Сохранение результатов

45. Сохраните результаты в файл `real_estate_analysis.xlsx` со следующими листами:
    - 'All_Properties': все объекты с новыми признаками
    - 'Premium': премиум недвижимость
    - 'By_Age': анализ по возрасту
    - 'By_Location': анализ по расположению
    - 'Price_Factors': факторы влияющие на цену
    - 'Investment_Guide': руководство для инвестиций

---

## Задача 24: Качество воздуха

**Датасет**: Air Quality Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/360/air+quality  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/360/air+quality.zip  
**Лицензия**: CC BY 4.0

**Описание**: Почасовые измерения качества воздуха с итальянской станции мониторинга (март 2004 - апрель 2005). Содержит 9358 записей. Столбцы: Date, Time, CO (угарный газ), PT08.S1 (датчик оксида олова), NMHC (неметановые углеводороды), C6H6 (бензол), PT08.S2, NOx (оксиды азота), PT08.S3, NO2 (диоксид азота), PT08.S4, PT08.S5, T (температура), RH (относительная влажность), AH (абсолютная влажность).

### Загрузка и первичный анализ

1. Загрузите датасет AirQualityUCI.csv (разделитель - точка с запятой)
2. Выведите первые 20 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета и временной охват

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений (значение -200.0 означает пропуск)
8. Замените -200.0 на NaN
9. Вычислите процент пропусков для каждого столбца
10. Заполните пропуски методом интерполяции (для временных рядов)
11. Проверьте наличие дубликатов по DateTime
12. Удалите строки с пропусками если их меньше 5%

### Преобразование типов данных

13. Объедините столбцы Date и Time в один столбец `DateTime`
14. Преобразуйте `DateTime` в формат datetime
15. Установите `DateTime` как индекс датафрейма
16. Замените запятые на точки в числовых столбцах (если нужно) и преобразуйте в float

### Создание новых признаков

17. Извлеките `year`, `month`, `day`, `hour` из DateTime
18. Создайте столбец `day_of_week` и `is_weekend`
19. Создайте столбец `season`: winter, spring, summer, fall
20. Создайте столбец `time_of_day`: night (0-6), morning (6-12), afternoon (12-18), evening (18-24)
21. Создайте столбец `air_quality_index` как взвешенную сумму загрязнителей
22. Создайте столбец `pollution_level`: good, moderate, unhealthy, very_unhealthy на основе AQI
23. Создайте столбец `temperature_category`: cold (<10°C), mild (10-20), warm (20-30), hot (>30)
24. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

25. Отфильтруйте часы с высоким загрязнением (pollution_level='unhealthy' или 'very_unhealthy')
26. Сохраните результат в переменную `high_pollution_hours`
27. Отфильтруйте летние месяцы (июнь-август)
28. Используя метод `query()`, найдите: `CO > 2.0 and C6H6 > 10.0 and hour >= 7 and hour <= 9`
29. Отсортируйте по уровню CO (от большего к меньшему)
30. Отсортируйте по дате
31. Найдите топ-100 часов с наихудшим качеством воздуха

### Группировка и агрегация

32. Сгруппируйте данные по `hour` и вычислите средние уровни всех загрязнителей
33. Для каждого часа дня найдите средний air_quality_index
34. Найдите час дня с наихудшим качеством воздуха
35. Сгруппируйте по `month` и вычислите:
    - Средние уровни CO, NO2, C6H6
    - Среднюю температуру и влажность
    - Процент часов с высоким загрязнением
    - Максимальные значения загрязнителей
36. Используя метод `agg()`, для каждого `season` вычислите:
```python
{
    'CO': ['mean', 'max', 'std'],
    'NO2': ['mean', 'max'],
    'C6H6': ['mean', 'max'],
    'T': ['mean', 'min', 'max'],
    'air_quality_index': ['mean', 'std']
}
```
37. Найдите дни недели с наихудшим качеством воздуха

### Сводные таблицы

38. Создайте сводную таблицу: `month` в строках, `hour` в столбцах, средний CO
39. Создайте сводную таблицу: `season` × `time_of_day` со средним air_quality_index
40. Создайте сводную таблицу с процентом высокого загрязнения: `day_of_week` × `hour`
41. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
42. Создайте таблицу `crosstab` для `season` и `pollution_level`

### Объединение датафреймов

43. Создайте датафрейм `daily_average` с дневными средними значениями (ресемплинг)
44. Создайте датафрейм `monthly_stats` с месячной статистикой
45. Объедините почасовые данные с дневными средними
46. Создайте датафреймы для рабочих дней и выходных
47. Объедините их для сравнительного анализа
48. Создайте итоговый датафрейм с трендами по месяцам

### Сохранение результатов

49. Сохраните результаты в файл `air_quality_analysis.xlsx` со следующими листами:
    - 'Hourly_Data': почасовые данные (первые 5000 строк)
    - 'High_Pollution': часы высокого загрязнения
    - 'Daily_Average': дневные средние
    - 'By_Hour': анализ по часам дня
    - 'By_Season': анализ по сезонам
    - 'Pollution_Patterns': паттерны загрязнения
    - 'Health_Report': отчет для здравоохранения

---

## Задача 25: Интернет-магазин (Online Retail)

**Датасет**: Online Retail Dataset  
**Источник**: UCI Machine Learning Repository  
**Прямая ссылка**: https://archive.ics.uci.edu/dataset/352/online+retail  
**Прямое скачивание**: https://archive.ics.uci.edu/static/public/352/online+retail.zip  
**Лицензия**: CC BY 4.0

**Описание**: Транзакции интернет-магазина из Великобритании (декабрь 2010 - декабрь 2011). Содержит 541909 записей. Столбцы: InvoiceNo (номер счета), StockCode (код товара), Description (описание), Quantity (количество), InvoiceDate (дата), UnitPrice (цена за единицу), CustomerID (ID клиента), Country (страна).

### Загрузка и первичный анализ

1. Загрузите датасет Online Retail.xlsx
2. Выведите первые 15 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Удалите строки с пропущенным CustomerID (невозможно идентифицировать клиента)
10. Удалите строки с пропущенным Description
11. Проверьте наличие отрицательных значений в Quantity (возвраты)
12. Создайте отдельный датафрейм для возвратов и удалите их из основного

### Преобразование типов данных

13. Преобразуйте столбец `InvoiceDate` в формат datetime
14. Преобразуйте `CustomerID` в строковый тип
15. Убедитесь, что Quantity и UnitPrice имеют числовой тип

### Создание новых признаков

16. Создайте столбец `TotalPrice` = `Quantity` × `UnitPrice`
17. Извлеките `year`, `month`, `day`, `hour`, `day_of_week` из InvoiceDate
18. Создайте столбец `is_weekend` (True для субботы и воскресенья)
19. Создайте столбец `quarter` (квартал года)
20. Создайте столбец `time_of_day`: morning, afternoon, evening
21. Создайте столбец `price_category`: cheap (<5), medium (5-20), expensive (>20)
22. Создайте столбец `bulk_purchase` (True если Quantity > 10)
23. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

24. Отфильтруйте только транзакции из Великобритании
25. Сохраните результат в переменную `uk_sales`
26. Отфильтруйте крупные покупки (TotalPrice > 1000)
27. Используя метод `query()`, найдите: `Quantity > 20 and UnitPrice > 10 and Country == "United Kingdom"`
28. Отсортируйте по TotalPrice (от большей к меньшей)
29. Отсортируйте по `InvoiceDate`
30. Найдите топ-100 самых дорогих транзакций

### Группировка и агрегация

31. Сгруппируйте данные по `Country` и подсчитайте количество транзакций
32. Для каждой страны вычислите общую выручку (сумму TotalPrice)
33. Найдите страну с максимальной выручкой
34. Сгруппируйте по `CustomerID` и вычислите:
    - Количество покупок
    - Общую потраченную сумму
    - Среднюю сумму чека
    - Количество уникальных товаров
    - Дату первой и последней покупки
35. Используя метод `agg()`, для каждого `month` вычислите:
```python
{
    'InvoiceNo': 'nunique',  # количество уникальных заказов
    'TotalPrice': ['sum', 'mean'],
    'Quantity': 'sum',
    'CustomerID': 'nunique'  # количество уникальных клиентов
}
```
36. Найдите топ-20 самых популярных товаров (по количеству продаж)
37. Найдите топ-20 самых прибыльных товаров (по выручке)

### Сводные таблицы

38. Создайте сводную таблицу: `month` в строках, `day_of_week` в столбцах, сумма TotalPrice
39. Создайте сводную таблицу: `Country` (топ-10) × `quarter` с выручкой
40. Создайте сводную таблицу со средним чеком: `time_of_day` × `is_weekend`
41. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
42. Создайте таблицу `crosstab` для `Country` и `bulk_purchase`

### Объединение датафреймов

43. Создайте датафрейм `customer_segments` с сегментацией клиентов (RFM анализ)
44. Создайте датафрейм `product_stats` со статистикой по каждому товару
45. Создайте датафрейм `monthly_revenue` с помесячной выручкой
46. Объедините основной датасет и `customer_segments` по CustomerID
47. Объедините с `product_stats` по StockCode
48. Создайте итоговый аналитический датафрейм

### Сохранение результатов

49. Сохраните результаты в файл `online_retail_analysis.xlsx` со следующими листами:
    - 'Clean_Sales': очищенные продажи (первые 10000 строк)
    - 'Top_Customers': топ-100 клиентов
    - 'Top_Products': топ-100 товаров
    - 'By_Country': анализ по странам
    - 'Monthly_Trends': помесячные тренды
    - 'Customer_Segments': сегментация клиентов
    - 'Business_Insights': бизнес-инсайты

---

Завершаю последними задачами 26-30.

---

## Задача 26: Отток сотрудников (Employee Attrition)

**Датасет**: IBM HR Analytics Employee Attrition Dataset  
**Источник**: Kaggle (IBM Watson Analytics)  
**Прямая ссылка**: https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset  
**Лицензия**: CC0 Public Domain

**Описание**: Синтетический датасет IBM для анализа оттока сотрудников. Содержит 1470 записей о сотрудниках с 35 признаками. Столбцы включают: Age, Attrition (увольнение), BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager.

### Загрузка и первичный анализ

1. Загрузите датасет WA_Fn-UseC_-HR-Employee-Attrition.csv
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных (типы столбцов, количество записей)
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета (количество строк и столбцов)

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца (их не должно быть)
9. Удалите столбцы с нулевой вариативностью (например, Over18, StandardHours, EmployeeCount)
10. Проверьте наличие полных дубликатов в датасете
11. Проверьте уникальность EmployeeNumber
12. Удалите дубликаты если есть

### Преобразование типов данных

13. Преобразуйте столбец `Attrition` в бинарный: Yes=1, No=0
14. Преобразуйте `OverTime` в бинарный: Yes=1, No=0
15. Преобразуйте категориальные столбцы в тип category

### Создание новых признаков

16. Создайте столбец `age_group`: young (<30), middle (30-45), senior (>45)
17. Создайте столбец `income_category`: low (<40k), medium (40-80k), high (>80k)
18. Создайте столбец `tenure_category`: new (<2 года), experienced (2-5), veteran (>5)
19. Создайте столбец `promotion_gap` = `YearsAtCompany` - `YearsSinceLastPromotion`
20. Создайте столбец `avg_satisfaction` = среднее из всех satisfaction scores
21. Создайте столбец `career_progression` = `JobLevel` / `TotalWorkingYears`
22. Создайте столбец `work_life_score` на основе WorkLifeBalance, OverTime, DistanceFromHome
23. Создайте столбец `retention_risk`: low, medium, high на основе комбинации факторов
24. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

25. Отфильтруйте сотрудников, которые уволились (Attrition=1)
26. Сохраните результат в переменную `departed_employees`
27. Отфильтруйте высокоэффективных сотрудников с низкой зарплатой
28. Используя метод `query()`, найдите: `MonthlyIncome < 5000 and PerformanceRating > 3 and Attrition == 1`
29. Отсортируйте по MonthlyIncome (от меньшего к большему)
30. Отсортируйте по `Department`, затем по `JobLevel`
31. Найдите топ-50 самых высокооплачиваемых сотрудников

### Группировка и агрегация

32. Сгруппируйте данные по `Department` и подсчитайте количество сотрудников
33. Для каждого департамента вычислите процент оттока (attrition rate)
34. Найдите департамент с наивысшим процентом оттока
35. Сгруппируйте по `JobRole` и вычислите:
    - Количество сотрудников
    - Процент оттока
    - Среднюю зарплату
    - Среднее удовлетворение работой
    - Средний стаж в компании
36. Используя метод `agg()`, для каждого `age_group` вычислите:
```python
{
    'Attrition': 'mean',
    'MonthlyIncome': ['mean', 'median', 'std'],
    'JobSatisfaction': 'mean',
    'WorkLifeBalance': 'mean',
    'YearsAtCompany': 'mean'
}
```
37. Найдите топ-5 факторов, наиболее коррелирующих с оттоком

### Сводные таблицы

38. Создайте сводную таблицу: `Department` в строках, `JobLevel` в столбцах, процент оттока
39. Создайте сводную таблицу: `age_group` × `income_category` со средним JobSatisfaction
40. Создайте сводную таблицу с attrition rate: `OverTime` × `WorkLifeBalance`
41. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
42. Создайте таблицу `crosstab` для `Department` и `Attrition`

### Объединение датафреймов

43. Создайте отдельный датафрейм `high_risk_employees` с сотрудниками высокого риска
44. Создайте датафрейм `department_stats` со статистикой по департаментам
45. Создайте датафрейм `role_benchmarks` с бенчмарками по ролям
46. Объедините основной датасет и `department_stats` используя `merge()` по `Department`
47. Объедините с `role_benchmarks` по `JobRole`
48. Разделите на уволившихся и оставшихся сотрудников
49. Объедините обратно используя `pd.concat()`

### Сохранение результатов

50. Сохраните результаты в файл `employee_attrition_analysis.xlsx` со следующими листами:
    - 'All_Employees': все сотрудники с новыми признаками
    - 'Departed': уволившиеся сотрудники
    - 'High_Risk': сотрудники группы риска
    - 'By_Department': анализ по департаментам
    - 'By_Role': анализ по ролям
    - 'Retention_Strategy': стратегия удержания
    - 'Predictive_Factors': предикторы оттока

---

## Задача 27: Обнаружение мошенничества с кредитными картами

**Датасет**: Credit Card Fraud Detection  
**Источник**: Kaggle (Machine Learning Group - ULB)  
**Прямая ссылка**: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud  
**Лицензия**: DbCL (Database Contents License)

**Описание**: Транзакции по кредитным картам европейских держателей за сентябрь 2013. Содержит 284,807 транзакций, из которых 492 - мошеннические (0.172%). Датасет сильно несбалансирован. Столбцы: Time (секунды с первой транзакции), V1-V28 (анонимизированные признаки после PCA), Amount (сумма транзакции), Class (0=нормальная, 1=мошенническая).

### Загрузка и первичный анализ

1. Загрузите датасет creditcard.csv
2. Выведите первые 10 строк датасета
3. Выведите информацию о структуре данных
4. Выведите описательную статистику для всех числовых столбцов
5. Выведите список всех колонок датасета
6. Проверьте размер датасета и соотношение классов

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце (их не должно быть)
8. Вычислите процент мошеннических транзакций
9. Проверьте наличие полных дубликатов в датасете
10. Проверьте дубликаты среди мошеннических транзакций отдельно
11. Удалите дубликаты если есть
12. Проверьте распределение признаков V1-V28

### Преобразование типов данных

13. Преобразуйте столбец `Class` в булевый тип (для удобства)
14. Убедитесь, что все числовые столбцы имеют тип float
15. Создайте копию датасета для дальнейших манипуляций

### Создание новых признаков

16. Создайте столбец `hour` из Time (Time / 3600 % 24)
17. Создайте столбец `day` из Time (Time / 86400)
18. Создайте столбец `time_of_day`: night, morning, afternoon, evening
19. Создайте столбец `amount_category`: micro (<10), small (10-100), medium (100-500), large (>500)
20. Создайте столбец `amount_log` = log(Amount + 1) для нормализации
21. Создайте столбец `amount_zscore` = стандартизированный Amount
22. Создайте столбец `v_sum` = сумма абсолютных значений V1-V28
23. Создайте столбец `v_mean` = среднее значение V1-V28
24. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

25. Отфильтруйте только мошеннические транзакции (Class=1)
26. Сохраните результат в переменную `fraud_transactions`
27. Отфильтруйте крупные транзакции (Amount > 1000)
28. Используя метод `query()`, найдите: `Amount > 500 and hour >= 0 and hour <= 6`
29. Отсортируйте мошеннические транзакции по сумме (от большей к меньшей)
30. Отсортируйте по `Time`
31. Найдите топ-100 самых крупных мошеннических транзакций

### Группировка и агрегация

32. Сгруппируйте данные по `hour` и подсчитайте количество транзакций
33. Для каждого часа вычислите процент мошеннических транзакций
34. Найдите час дня с наивысшим риском мошенничества
35. Сгруппируйте по `amount_category` и вычислите:
    - Общее количество транзакций
    - Количество мошеннических транзакций
    - Процент мошенничества
    - Среднюю сумму
    - Средние значения V1, V2, V3 (примеры признаков)
36. Используя метод `agg()`, для каждого `time_of_day` вычислите:
```python
{
    'Class': ['sum', 'mean'],  # количество и процент мошенничества
    'Amount': ['sum', 'mean', 'max'],
    'V1': 'mean',
    'V2': 'mean'
}
```
37. Найдите признаки V1-V28 с наибольшей разницей между мошенническими и нормальными транзакциями

### Сводные таблицы

38. Создайте сводную таблицу: `day` (первые 7 дней) в строках, `hour` в столбцах, количество мошенничеств
39. Создайте сводную таблицу: `amount_category` × `time_of_day` с процентом мошенничества
40. Создайте сводную таблицу со средней суммой: `hour` × `Class`
41. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
42. Создайте таблицу `crosstab` для `amount_category` и `Class`

### Объединение датафреймов

43. Создайте отдельный датафрейм `fraud_patterns` с анализом паттернов мошенничества
44. Создайте датафрейм `hourly_stats` со статистикой по часам
45. Создайте датафрейм `normal_sample` - случайная выборка нормальных транзакций для баланса
46. Объедините `fraud_transactions` и `normal_sample` для создания сбалансированного датасета
47. Разделите исходный датасет на train и test (по времени)
48. Объедините статистику с основным датасетом

### Сохранение результатов

49. Сохраните результаты в файл `fraud_detection_analysis.xlsx` со следующими листами:
    - 'Fraud_Transactions': все мошеннические транзакции
    - 'Large_Frauds': крупное мошенничество
    - 'Hourly_Analysis': анализ по часам
    - 'Amount_Analysis': анализ по суммам
    - 'Feature_Importance': важность признаков
    - 'Detection_Patterns': паттерны обнаружения
    - 'Risk_Profile': профиль риска

---

## Задача 28: Анализ фондового рынка

**Датасет**: Stock Market Data (S&P 500)  
**Источник**: Kaggle / Yahoo Finance  
**Прямая ссылка**: https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs  
**Альтернатива**: Можно загрузить через yfinance библиотеку для конкретных акций  
**Лицензия**: Public Domain / Fair Use

**Описание**: Исторические данные о ценах акций. Типичный датасет содержит: Date (дата), Open (цена открытия), High (максимум), Low (минимум), Close (цена закрытия), Volume (объем торгов), Adj Close (скорректированная цена закрытия). Для этой задачи используйте данные по нескольким акциям или индексу.

### Загрузка и первичный анализ

1. Загрузите датасет для нескольких акций (например, AAPL, GOOGL, MSFT, AMZN)
2. Объедините их в один датафрейм с мультииндексом или отдельным столбцом Symbol
3. Выведите первые 20 строк датасета
4. Выведите информацию о структуре данных
5. Выведите описательную статистику для всех числовых столбцов
6. Проверьте размер датасета и временной период

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце
8. Вычислите процент пропусков для каждого столбца
9. Заполните пропуски в ценах методом forward fill
10. Заполните пропуски в Volume нулями или средним значением
11. Проверьте наличие дубликатов по Date и Symbol
12. Удалите дубликаты если есть

### Преобразование типов данных

13. Преобразуйте столбец `Date` в формат datetime
14. Установите `Date` как индекс датафрейма (или часть мультииндекса)
15. Убедитесь, что все ценовые столбцы имеют тип float

### Создание новых признаков

16. Создайте столбец `daily_return` = (Close - Close.shift(1)) / Close.shift(1) * 100
17. Создайте столбец `intraday_range` = High - Low
18. Создайте столбец `price_change` = Close - Open
19. Создайте столбец `volatility` = (High - Low) / Open * 100
20. Создайте столбец `volume_change` = (Volume - Volume.shift(1)) / Volume.shift(1) * 100
21. Создайте столбец `MA_7` = скользящее среднее Close за 7 дней
22. Создайте столбец `MA_30` = скользящее среднее Close за 30 дней
23. Создайте столбец `trend` = 'up' если MA_7 > MA_30, иначе 'down'
24. Создайте столбец `RSI` (Relative Strength Index) за 14 дней
25. Извлеките `year`, `month`, `quarter`, `day_of_week` из Date
26. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

27. Отфильтруйте данные за последний год
28. Сохраните результат в переменную `recent_data`
29. Отфильтруйте дни с высокой волатильностью (volatility > 5%)
30. Используя метод `query()`, найдите: `daily_return > 5 and Volume > 50000000`
31. Отсортируйте по daily_return (от большего к меньшему)
32. Отсортируйте по `Symbol`, затем по `Date`
33. Найдите топ-50 дней с наибольшим объемом торгов

### Группировка и агрегация

34. Сгруппируйте данные по `Symbol` и вычислите общую статистику
35. Для каждой акции вычислите среднюю доходность и волатильность
36. Найдите акцию с наивысшей средней доходностью
37. Сгруппируйте по `year` и `Symbol` и вычислите:
    - Цену на начало года (первый Close)
    - Цену на конец года (последний Close)
    - Годовую доходность
    - Среднюю волатильность
    - Максимальную и минимальную цены
38. Используя метод `agg()`, для каждого `month` вычислите:
```python
{
    'Close': ['first', 'last', 'min', 'max'],
    'daily_return': ['mean', 'std'],
    'Volume': 'sum',
    'volatility': 'mean'
}
```
39. Найдите месяцы с наилучшей доходностью для каждой акции

### Сводные таблицы

40. Создайте сводную таблицу: `year` в строках, `Symbol` в столбцах, годовая доходность
41. Создайте сводную таблицу: `month` × `Symbol` со средним daily_return
42. Создайте сводную таблицу со средней волатильностью: `quarter` × `Symbol`
43. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
44. Создайте таблицу сравнения производительности акций

### Объединение датафреймов

45. Создайте датафрейм `annual_performance` с годовой производительностью каждой акции
46. Создайте датафрейм `correlation_matrix` с корреляциями между акциями
47. Создайте датафрейм `portfolio` с весами акций в портфеле
48. Объедините дневные данные с annual_performance
49. Вычислите доходность портфеля на основе весов
50. Создайте итоговый датафрейм для инвестиционного анализа

### Сохранение результатов

51. Сохраните результаты в файл `stock_market_analysis.xlsx` со следующими листами:
    - 'Daily_Data': дневные данные (последние 500 дней)
    - 'Annual_Performance': годовая производительность
    - 'Monthly_Returns': помесячная доходность
    - 'Volatility_Analysis': анализ волатильности
    - 'Correlation_Matrix': корреляции между акциями
    - 'Portfolio_Analysis': анализ портфеля
    - 'Investment_Report': инвестиционный отчет

---

## Задача 29: Анализ задержек авиарейсов

**Датасет**: Flight Delays Dataset  
**Источник**: Kaggle / US DOT  
**Прямая ссылка**: https://www.kaggle.com/datasets/usdot/flight-delays  
**Лицензия**: Public Domain

**Описание**: Данные о задержках авиарейсов в США за 2015 год. Содержит ~5.8 миллионов записей. Три основных файла: flights.csv (рейсы), airlines.csv (авиакомпании), airports.csv (аэропорты). Столбцы flights.csv включают: YEAR, MONTH, DAY, DAY_OF_WEEK, AIRLINE, FLIGHT_NUMBER, TAIL_NUMBER, ORIGIN_AIRPORT, DESTINATION_AIRPORT, SCHEDULED_DEPARTURE, DEPARTURE_TIME, DEPARTURE_DELAY, TAXI_OUT, WHEELS_OFF, SCHEDULED_TIME, ELAPSED_TIME, AIR_TIME, DISTANCE, WHEELS_ON, TAXI_IN, SCHEDULED_ARRIVAL, ARRIVAL_TIME, ARRIVAL_DELAY, DIVERTED, CANCELLED, CANCELLATION_REASON, AIR_SYSTEM_DELAY, SECURITY_DELAY, AIRLINE_DELAY, LATE_AIRCRAFT_DELAY, WEATHER_DELAY.

### Загрузка и первичный анализ

1. Загрузите три датасета: flights.csv, airlines.csv, airports.csv
2. Для flights.csv загрузите только первые 100,000 строк (или используйте чанки)
3. Выведите первые 10 строк каждого датасета
4. Выведите информацию о структуре данных flights
5. Выведите описательную статистику для числовых столбцов
6. Проверьте размеры всех трех датасетов

### Обработка пропусков и дубликатов

7. Подсчитайте количество пропущенных значений в каждом столбце flights
8. Вычислите процент пропусков для ключевых столбцов
9. Заполните пропуски в ARRIVAL_DELAY и DEPARTURE_DELAY нулями (для отмененных рейсов)
10. Удалите строки где и DEPARTURE_TIME и ARRIVAL_TIME пустые
11. Проверьте наличие дубликатов
12. Удалите дубликаты если есть

### Преобразование типов данных

13. Создайте столбец `DATE` объединяя YEAR, MONTH, DAY
14. Преобразуйте DATE в формат datetime
15. Преобразуйте CANCELLED и DIVERTED в булевый тип
16. Преобразуйте категориальные столбцы в тип category

### Создание новых признаков

17. Создайте столбец `is_delayed` (True если DEPARTURE_DELAY > 15)
18. Создайте столбец `delay_category`: on_time (<0), minor (0-30), moderate (30-60), severe (>60)
19. Создайте столбец `time_of_day`: early_morning, morning, afternoon, evening, night на основе SCHEDULED_DEPARTURE
20. Создайте столбец `route` = ORIGIN_AIRPORT + '-' + DESTINATION_AIRPORT
21. Создайте столбец `distance_category`: short (<500), medium (500-1500), long (>1500)
22. Создайте столбец `total_delay` = сумма всех типов задержек
23. Создайте столбец `main_delay_reason` - причина с максимальной задержкой
24. Создайте столбец `weekend` (True если DAY_OF_WEEK in [6,7])
25. Выведите первые 10 строк с новыми столбцами

### Фильтрация и сортировка данных

26. Отфильтруйте только задержанные рейсы (is_delayed=True)
27. Сохраните результат в переменную `delayed_flights`
28. Отфильтруйте отмененные рейсы
29. Используя метод `query()`, найдите: `DEPARTURE_DELAY > 120 and DISTANCE > 1000`
30. Отсортируйте по DEPARTURE_DELAY (от большей к меньшей)
31. Отсортируйте по `AIRLINE`, затем по `DEPARTURE_DELAY`
32. Найдите топ-100 самых задержанных рейсов

### Группировка и агрегация

33. Сгруппируйте данные по `AIRLINE` и подсчитайте количество рейсов
34. Для каждой авиакомпании вычислите процент задержанных рейсов
35. Найдите авиакомпанию с наихудшей пунктуальностью
36. Сгруппируйте по `ORIGIN_AIRPORT` (топ-20) и вычислите:
    - Количество рейсов
    - Процент задержанных
    - Среднюю задержку вылета
    - Процент отмененных рейсов
    - Среднее расстояние
37. Используя метод `agg()`, для каждого `time_of_day` вычислите:
```python
{
    'DEPARTURE_DELAY': ['mean', 'median', 'max'],
    'is_delayed': 'mean',
    'CANCELLED': 'mean',
    'DISTANCE': 'mean'
}
```
38. Найдите топ-10 маршрутов с наибольшими задержками

### Сводные таблицы

39. Создайте сводную таблицу: `AIRLINE` в строках, `MONTH` в столбцах, процент задержек
40. Создайте сводную таблицу: `distance_category` × `time_of_day` со средней задержкой
41. Создайте сводную таблицу причин отмен: `AIRLINE` × `CANCELLATION_REASON`
42. Добавьте итоговые значения к сводной таблице (параметр `margins=True`)
43. Создайте таблицу `crosstab` для `DAY_OF_WEEK` и `is_delayed`

### Объединение датафреймов

44. Объедините flights и airlines по AIRLINE (добавьте полное название авиакомпании)
45. Объедините с airports по ORIGIN_AIRPORT (добавьте информацию об аэропорте отправления)
46. Объедините с airports по DESTINATION_AIRPORT (добавьте информацию об аэропорте назначения)
47. Создайте датафрейм `airline_performance` с производительностью авиакомпаний
48. Создайте датафрейм `airport_stats` со статистикой по аэропортам
49. Создайте итоговый аналитический датафрейм со всей информацией

### Сохранение результатов

50. Сохраните результаты в файл `flight_delays_analysis.xlsx` со следующими листами:
    - 'Delayed_Flights': задержанные рейсы (первые 5000)
    - 'Cancelled_Flights': отмененные рейсы
    - 'Airline_Performance': производительность авиакомпаний
    - 'Airport_Analysis': анализ по аэропортам
    - 'Delay_Patterns': паттерны задержек
    - 'Time_Analysis': анализ по времени суток
    - 'Executive_Summary': итоговый отчет

---

## Задача 30: Комплексное объединение датасетов - Мульти-источниковый анализ

**Датасеты**: Используйте 3-5 датасетов из предыдущих задач  
**Рекомендация**: Например, Titanic + World Happiness + Students Performance + Crime Data  
**Цель**: Продемонстрировать мастерство в объединении, трансформации и анализе данных из разных источников

### Концепция задачи

Эта задача фокусируется на продвинутых техниках объединения датафреймов, создании единого аналитического пайплайна и интеграции данных из множественных источников. Цель - создать комплексный аналитический проект.

### Часть 1: Загрузка и подготовка множественных датасетов

1. Загрузите минимум 3 различных датасета из предыдущих задач
2. Для каждого датасета выполните базовую очистку (пропуски, дубликаты)
3. Стандартизируйте названия столбцов (единый формат: lower_case_with_underscores)
4. Создайте единый идентификатор для каждого датасета (dataset_id)
5. Добавьте столбец source_dataset в каждый датафрейм
6. Выведите сводную информацию по всем загруженным датасетам

### Часть 2: Вертикальное объединение (concat)

7. Выберите общие признаки из разных датасетов (например, age, gender)
8. Создайте подмножества датафреймов только с общими столбцами
9. Объедините эти подмножества вертикально используя `pd.concat()`
10. Добавьте параметр `keys` для создания мультииндекса
11. Используйте параметр `ignore_index=True` для создания непрерывного индекса
12. Проверьте результат объединения

### Часть 3: Горизонтальное объединение (merge и join)

13. Создайте искусственный ключ для связи датасетов (например, country_code)
14. Выполните inner join между двумя датасетами
15. Выполните left join для сохранения всех записей из основного датасета
16. Выполните right join для анализа
17. Выполните outer join для получения всех данных
18. Используйте merge с параметром `indicator=True` для отслеживания источника
19. Выполните merge по множественным ключам

### Часть 4: Сложные объединения

20. Создайте связующую таблицу (bridge table) между датасетами
21. Выполните последовательность объединений (chain merges)
22. Используйте `pd.merge_asof()` для объединения по ближайшим значениям
23. Примените `pd.merge_ordered()` для упорядоченного объединения
24. Обработайте конфликты имен столбцов (suffixes='_left', '_right')
25. Проверьте качество объединений (количество потерянных записей)

### Часть 5: Трансформация объединенных данных

26. Создайте сводные статистики по каждому исходному датасету
27. Вычислите агрегированные метрики на объединенных данных
28. Создайте новые признаки на основе данных из разных источников
29. Нормализуйте числовые признаки для сравнимости
30. Создайте категориальные признаки из числовых (бинирование)

### Часть 6: Многоуровневый анализ

31. Сгруппируйте объединенные данные по source_dataset и вычислите статистику
32. Создайте мультииндекс группировку (например, по датасету и категории)
33. Используйте `groupby()` с множественными уровнями
34. Примените сложную агрегацию с различными функциями для разных столбцов
35. Используйте метод `transform()` для добавления групповой статистики

### Часть 7: Сводные таблицы на объединенных данных

36. Создайте мультиуровневую сводную таблицу с данными из всех источников
37. Создайте сводную таблицу с множественными значениями (values)
38. Используйте параметр `aggfunc` с несколькими функциями
39. Создайте `crosstab` для анализа взаимосвязей между датасетами
40. Добавьте subtotals и grand totals

### Часть 8: Временной анализ (если применимо)

41. Выровняйте временные ряды из разных датасетов
42. Выполните ресемплинг до общего временного периода
43. Заполните пропуски в временных рядах различными методами
44. Создайте лаговые признаки из разных источников
45. Вычислите скользящие статистики по объединенным данным

### Часть 9: Создание мастер-датасета

46. Объедините все предыдущие трансформации в единый пайплайн
47. Создайте финальный мастер-датасет со всеми признаками
48. Задокументируйте происхождение каждого столбца
49. Создайте data dictionary (словарь данных) с описаниями
50. Проверьте целостность и консистентность финального датасета

### Часть 10: Визуализация связей и экспорт

51. Создайте матрицу корреляций между признаками из разных источников
52. Создайте сравнительные статистики по датасетам
53. Подготовьте executive summary с ключевыми инсайтами
54. Экспортируйте результаты в многолистовой Excel файл
55. Создайте отдельные CSV файлы для каждого этапа анализа

### Сохранение результатов

56. Сохраните результаты в файл `multi_source_analysis.xlsx` со следующими листами:
    - 'Master_Dataset': финальный объединенный датасет (первые 10000 строк)
    - 'Data_Dictionary': описание всех столбцов и источников
    - 'Source_Summary': сводка по каждому исходному датасету
    - 'Merge_Quality': отчет о качестве объединений
    - 'Cross_Dataset_Stats': статистика по датасетам
    - 'Correlation_Matrix': корреляции между признаками
    - 'Key_Insights': ключевые находки из объединенного анализа
    - 'Methodology': описание методологии объединения

### Дополнительные задания (бонус)

57. Создайте функцию для автоматического объединения новых датасетов
58. Напишите валидацию для проверки качества объединений
59. Создайте пайплайн для воспроизводимого анализа
60. Экспортируйте код в модульную структуру (.py файлы)

---

