# Домашнее задание 2: Разведочный анализ юридических данных (EDA)

## Описание задания

Теперь, когда вы нашли набор юридических данных, пришло время провести первичный разведочный анализ (Exploratory Data Analysis, EDA). Вы загрузите данные в pandas, изучите их структуру, проверите качество, посчитаете описательные статистики и создадите визуализации для понимания основных закономерностей.

**Дедлайн:** [указать дату]  
**Баллы:** [указать количество]

---

## Цели задания

1. Научиться загружать данные в pandas и проводить первичный осмотр
2. Выявлять и анализировать проблемы качества данных (пропуски, выбросы)
3. Вычислять описательные статистики для числовых и категориальных переменных
4. Анализировать взаимосвязи между переменными
5. Создавать информативные визуализации для презентации данных

---

## Задачи

### Часть 1: Загрузка и первичный осмотр данных (обязательно)

#### 1.1. Загрузите данные

Загрузите ваш набор данных в pandas DataFrame. В зависимости от формата используйте:
```python
import pandas as pd
import numpy as np

# Для CSV
df = pd.read_csv('your_data.csv')

# Для Excel
df = pd.read_excel('your_data.xlsx')

# Для JSON
df = pd.read_json('your_data.json')
```

**Что нужно сделать:**
- Загрузите данные и сохраните в переменную `df`
- Если есть проблемы с кодировкой, укажите параметр `encoding` (например, `encoding='utf-8'` или `encoding='cp1251'`)
- Если файл большой, можно загрузить первые 10000 строк: `pd.read_csv('data.csv', nrows=10000)`

#### 1.2. Общая информация о данных

Получите базовую информацию о датасете:

```python
# Размерность данных
print(f"Количество строк: {df.shape[0]}")
print(f"Количество столбцов: {df.shape[1]}")

# Первые строки
print(df.head())

# Последние строки
print(df.tail())

# Информация о типах данных и пропусках
print(df.info())

# Названия всех столбцов
print(df.columns.tolist())
```

**Создайте таблицу с информацией о данных:**

| Показатель | Значение |
|------------|----------|
| Количество наблюдений (строк) | |
| Количество переменных (столбцов) | |
| Размер данных в памяти (MB) | |
| Количество дубликатов строк | |

Для подсчета дубликатов: `df.duplicated().sum()`

#### 1.3. Типы данных

Создайте таблицу со всеми переменными и их типами данных в pandas:

```python
# Типы данных
dtypes_df = pd.DataFrame({
    'Название переменной': df.columns,
    'Тип в pandas': df.dtypes.values,
    'Количество уникальных значений': [df[col].nunique() for col in df.columns]
})
print(dtypes_df)
```

**Проанализируйте:**
- Все ли типы данных определены корректно?
- Нужно ли преобразовать какие-то столбцы? (например, даты хранятся как строки)
- Есть ли переменные, которые должны быть категориальными?

**Если нужны преобразования, выполните их:**
```python
# Преобразование в datetime
df['date_column'] = pd.to_datetime(df['date_column'])

# Преобразование в категориальный тип
df['category_column'] = df['category_column'].astype('category')

# Преобразование в числовой тип
df['numeric_column'] = pd.to_numeric(df['numeric_column'], errors='coerce')
```

---

### Часть 2: Анализ пропущенных значений (обязательно)

#### 2.1. Подсчет пропусков

Проверьте наличие пропущенных значений:

```python
# Количество пропусков по каждому столбцу
missing_data = pd.DataFrame({
    'Переменная': df.columns,
    'Количество пропусков': df.isnull().sum().values,
    'Процент пропусков': (df.isnull().sum().values / len(df) * 100).round(2)
})

# Сортируем по убыванию процента пропусков
missing_data = missing_data.sort_values('Процент пропусков', ascending=False)
print(missing_data[missing_data['Количество пропусков'] > 0])
```

#### 2.2. Визуализация пропусков

Создайте визуализацию пропущенных значений:

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Настройка стиля
plt.rcParams['figure.figsize'] = (10, 6)
sns.set_style("whitegrid")

# Если есть пропуски
if df.isnull().sum().sum() > 0:
    # График пропусков
    plt.figure(figsize=(10, 6))
    missing_cols = missing_data[missing_data['Количество пропусков'] > 0]
    plt.barh(missing_cols['Переменная'], missing_cols['Процент пропусков'])
    plt.xlabel('Процент пропусков (%)')
    plt.title('Пропущенные значения по переменным')
    plt.tight_layout()
    plt.show()
```

#### 2.3. Анализ пропусков

**Ответьте на вопросы:**
1. В каких переменных больше всего пропусков?
2. Есть ли закономерность в пропусках? (например, пропуски связаны между собой)
3. Как вы планируете обрабатывать пропуски? (удалить строки, заполнить средним/медианой/модой, оставить как есть)

**Пример проверки связанности пропусков:**
```python
# Проверка: если пропущена переменная A, пропущена ли и переменная B?
df[['column_A', 'column_B']].isnull().sum()
```

---

### Часть 3: Описательные статистики (обязательно)

#### 3.1. Статистики для числовых переменных

Рассчитайте базовые статистики для всех числовых переменных:

```python
# Описательные статистики
numeric_stats = df.describe()
print(numeric_stats)

# Дополнительные статистики
numeric_cols = df.select_dtypes(include=[np.number]).columns

for col in numeric_cols:
    print(f"\n=== {col} ===")
    print(f"Среднее: {df[col].mean():.2f}")
    print(f"Медиана: {df[col].median():.2f}")
    print(f"Мода: {df[col].mode().values[0] if not df[col].mode().empty else 'Нет'}")
    print(f"Стандартное отклонение: {df[col].std():.2f}")
    print(f"Минимум: {df[col].min():.2f}")
    print(f"Максимум: {df[col].max():.2f}")
    print(f"Квартили (25%, 50%, 75%): {df[col].quantile(0.25):.2f}, {df[col].quantile(0.5):.2f}, {df[col].quantile(0.75):.2f}")
```

**Создайте таблицу с ключевыми статистиками** (можно экспортировать из `describe()`):

| Переменная | Count | Mean | Std | Min | 25% | 50% | 75% | Max |
|------------|-------|------|-----|-----|-----|-----|-----|-----|
| ... | ... | ... | ... | ... | ... | ... | ... | ... |

#### 3.2. Статистики для категориальных переменных

Проанализируйте категориальные переменные:

```python
# Выбираем категориальные столбцы
categorical_cols = df.select_dtypes(include=['object', 'category']).columns

for col in categorical_cols:
    print(f"\n=== {col} ===")
    print(f"Количество уникальных значений: {df[col].nunique()}")
    print(f"Наиболее частое значение: {df[col].mode().values[0] if not df[col].mode().empty else 'Нет'}")
    print("\nТоп-10 наиболее частых значений:")
    print(df[col].value_counts().head(10))
    print("\nДоля топ-10 значений: {:.1f}%".format(
        df[col].value_counts().head(10).sum() / len(df) * 100
    ))
```

**Для каждой категориальной переменной укажите:**
- Количество уникальных категорий
- Топ-3 наиболее частых значений и их частоты
- Есть ли редкие категории (встречаются менее 1% случаев)?

#### 3.3. Временной анализ (если есть даты)

Если в данных есть временные переменные:

```python
# Убедитесь, что столбец в формате datetime
df['date_column'] = pd.to_datetime(df['date_column'])

# Временной диапазон
print(f"Период данных: с {df['date_column'].min()} по {df['date_column'].max()}")
print(f"Длительность: {(df['date_column'].max() - df['date_column'].min()).days} дней")

# Количество наблюдений по годам/месяцам
df['year'] = df['date_column'].dt.year
df['month'] = df['date_column'].dt.month
print(df['year'].value_counts().sort_index())
```

---

### Часть 4: Корреляционный анализ (обязательно)

#### 4.1. Корреляционная матрица

Постройте корреляционную матрицу для числовых переменных:

```python
# Корреляционная матрица
correlation_matrix = df[numeric_cols].corr()
print(correlation_matrix)

# Визуализация корреляционной матрицы
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            fmt='.2f', square=True, linewidths=1)
plt.title('Корреляционная матрица числовых переменных')
plt.tight_layout()
plt.show()
```

#### 4.2. Анализ корреляций

**Ответьте на вопросы:**
1. Какие переменные имеют **сильную положительную корреляцию** (> 0.7)?
2. Какие переменные имеют **сильную отрицательную корреляцию** (< -0.7)?
3. Есть ли неожиданные корреляции? Как их можно объяснить?
4. Есть ли переменные почти без корреляций с другими?

**Пример анализа:**
```python
# Найдем пары с высокой корреляцией
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.7:
            high_corr_pairs.append({
                'Переменная 1': correlation_matrix.columns[i],
                'Переменная 2': correlation_matrix.columns[j],
                'Корреляция': correlation_matrix.iloc[i, j]
            })

high_corr_df = pd.DataFrame(high_corr_pairs)
print(high_corr_df)
```

---

### Часть 5: Визуализация данных (обязательно)

#### 5.1. Распределение числовых переменных

Постройте гистограммы для всех числовых переменных:

```python
# Гистограммы для всех числовых переменных
df[numeric_cols].hist(bins=30, figsize=(15, 10), edgecolor='black')
plt.tight_layout()
plt.suptitle('Распределение числовых переменных', y=1.02, fontsize=16)
plt.show()

# Boxplot для выявления выбросов
fig, axes = plt.subplots(1, len(numeric_cols), figsize=(15, 5))
if len(numeric_cols) == 1:
    axes = [axes]
for i, col in enumerate(numeric_cols):
    df.boxplot(column=col, ax=axes[i])
    axes[i].set_title(col)
plt.tight_layout()
plt.show()
```

**Для каждой числовой переменной опишите:**
- Тип распределения (нормальное, скошенное вправо/влево, равномерное)
- Наличие выбросов
- Есть ли подозрительные значения (например, отрицательные суммы)?

#### 5.2. Распределение категориальных переменных

Постройте столбчатые диаграммы для категориальных переменных:

```python
# Для каждой категориальной переменной
for col in categorical_cols[:4]:  # Первые 4 переменные
    plt.figure(figsize=(10, 6))
    
    # Топ-10 категорий
    top_categories = df[col].value_counts().head(10)
    
    plt.barh(range(len(top_categories)), top_categories.values)
    plt.yticks(range(len(top_categories)), top_categories.index)
    plt.xlabel('Количество наблюдений')
    plt.title(f'Топ-10 категорий: {col}')
    plt.tight_layout()
    plt.show()
```

**Проанализируйте:**
- Какие категории доминируют?
- Есть ли сбалансированность или сильный дисбаланс?
- Нужно ли объединить редкие категории в группу "Другое"?

#### 5.3. Временные тренды (если есть даты)

Если в данных есть временные переменные, постройте графики изменения во времени:

```python
# Количество наблюдений по времени
time_counts = df.groupby('date_column').size()

plt.figure(figsize=(12, 6))
plt.plot(time_counts.index, time_counts.values)
plt.xlabel('Дата')
plt.ylabel('Количество наблюдений')
plt.title('Динамика количества наблюдений во времени')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Агрегация по месяцам/годам
monthly_counts = df.groupby(df['date_column'].dt.to_period('M')).size()
plt.figure(figsize=(12, 6))
monthly_counts.plot(kind='bar')
plt.xlabel('Месяц')
plt.ylabel('Количество наблюдений')
plt.title('Количество наблюдений по месяцам')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

#### 5.4. Взаимосвязи между переменными

Постройте графики для анализа взаимосвязей:

**А) Scatter plot для числовых переменных:**
```python
# Выберите 2-3 пары переменных с интересными корреляциями
plt.figure(figsize=(10, 6))
plt.scatter(df['variable_1'], df['variable_2'], alpha=0.5)
plt.xlabel('Переменная 1')
plt.ylabel('Переменная 2')
plt.title('Взаимосвязь между переменными')
plt.tight_layout()
plt.show()
```

**Б) Boxplot для числовой переменной по категориям:**
```python
# Например, суммы исков по типам дел
plt.figure(figsize=(12, 6))
df.boxplot(column='numeric_variable', by='categorical_variable', figsize=(12, 6))
plt.xlabel('Категория')
plt.ylabel('Числовая переменная')
plt.title('Распределение числовой переменной по категориям')
plt.suptitle('')  # Убираем автоматический заголовок pandas
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

**В) Grouped bar chart:**
```python
# Например, количество дел по типам и годам
crosstab = pd.crosstab(df['categorical_1'], df['categorical_2'])
crosstab.plot(kind='bar', figsize=(12, 6))
plt.xlabel('Категория 1')
plt.ylabel('Количество')
plt.title('Распределение по двум категориальным переменным')
plt.legend(title='Категория 2')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

**Постройте минимум 5 различных визуализаций**, которые раскрывают интересные закономерности в ваших данных.

---

### Часть 6: Выявление аномалий и проблем (обязательно)

#### 6.1. Поиск выбросов

Для числовых переменных найдите выбросы:

```python
# Метод межквартильного размаха (IQR)
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    
    print(f"\n=== {col} ===")
    print(f"Границы: [{lower_bound:.2f}, {upper_bound:.2f}]")
    print(f"Количество выбросов: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")
    
    if len(outliers) > 0:
        print(f"Примеры выбросов: {outliers[col].head().values}")
```

#### 6.2. Проверка на логические ошибки

**Проверьте данные на логические противоречия:**
- Есть ли отрицательные значения там, где их быть не должно? (суммы, сроки)
- Есть ли даты из будущего или слишком старые?
- Есть ли нереалистичные значения? (например, срок рассмотрения дела 10000 дней)
- Корректны ли комбинации категорий?

```python
# Примеры проверок
print("Отрицательные суммы:", (df['amount'] < 0).sum())
print("Даты из будущего:", (df['date'] > pd.Timestamp.now()).sum())
print("Нереалистично большие сроки:", (df['duration_days'] > 3650).sum())
```

#### 6.3. Анализ дублирующихся записей

```python
# Полные дубликаты
duplicates = df.duplicated()
print(f"Количество дублирующихся строк: {duplicates.sum()}")

# Дубликаты по ключевым полям
duplicates_by_key = df.duplicated(subset=['key_column_1', 'key_column_2'])
print(f"Дубликаты по ключевым полям: {duplicates_by_key.sum()}")

# Показать примеры дубликатов
if duplicates.sum() > 0:
    print("\nПримеры дублирующихся строк:")
    print(df[duplicates].head())
```

---

### Часть 7: Выводы и рекомендации (обязательно)

На основе проведенного анализа ответьте на следующие вопросы:

#### 7.1. Качество данных
- Каково общее качество данных? (отличное / хорошее / удовлетворительное / плохое)
- Какие основные проблемы качества вы обнаружили?
- Какие переменные требуют очистки или преобразования?

#### 7.2. Ключевые находки
Опишите **3-5 наиболее интересных находок** из анализа:
- Какие неожиданные закономерности вы обнаружили?
- Есть ли тренды во времени?
- Какие взаимосвязи между переменными наиболее сильные?
- Что говорят описательные статистики о природе данных?

#### 7.3. Рекомендации по дальнейшей работе
- Какие переменные наиболее перспективны для анализа?
- Нужна ли дополнительная предобработка? Какая именно?
- Какие гипотезы можно проверить на этих данных?
- Какие дополнительные данные были бы полезны?

---

## Формат сдачи

Создайте **Jupyter Notebook** (.ipynb) со следующей структурой:

```
# ДЗ 2: Разведочный анализ данных (EDA)
## ФИО, группа

## 1. Загрузка и первичный осмотр
[Код загрузки данных]
[Таблица с базовой информацией]
[Таблица с типами данных]

## 2. Анализ пропущенных значений
[Код проверки пропусков]
[Таблица и визуализация пропусков]
[Анализ и выводы]

## 3. Описательные статистики
### 3.1. Числовые переменные
[Код и таблицы статистик]
[Интерпретация результатов]

### 3.2. Категориальные переменные
[Код и таблицы частот]
[Анализ распределений]

### 3.3. Временной анализ (если применимо)
[Код и графики]

## 4. Корреляционный анализ
[Корреляционная матрица: код и heatmap]
[Таблица сильных корреляций]
[Интерпретация]

## 5. Визуализация
### 5.1. Распределения числовых переменных
[Гистограммы и boxplot'ы с описанием]

### 5.2. Распределения категориальных переменных
[Столбчатые диаграммы с анализом]

### 5.3. Временные тренды (если применимо)
[Графики временных рядов]

### 5.4. Взаимосвязи между переменными
[Минимум 5 различных визуализаций с комментариями]

## 6. Выявление аномалий
[Код поиска выбросов]
[Проверка логических ошибок]
[Анализ дубликатов]

## 7. Выводы
### 7.1. Качество данных
[Оценка и описание проблем]

### 7.2. Ключевые находки
[3-5 основных наблюдений]

### 7.3. Рекомендации
[План дальнейшей работы]
```

**Требования к notebook:**
- Весь код должен выполняться без ошибок
- Каждая ячейка кода должна сопровождаться текстовым комментарием с интерпретацией
- Визуализации должны иметь подписи осей и заголовки
- Графики должны быть читаемыми (подходящий размер, шрифты)
- Выводы должны быть конкретными и опираться на результаты анализа

---

## Критерии оценки

| Критерий | Баллы | Описание |
|----------|-------|----------|
| **Загрузка и первичный осмотр** | 10% | Данные корректно загружены, представлена базовая информация |
| **Анализ пропусков** | 15% | Полный анализ пропусков с визуализацией и обоснованными выводами |
| **Описательные статистики** | 20% | Рассчитаны статистики для всех типов переменных с интерпретацией |
| **Корреляционный анализ** | 15% | Построена матрица, выявлены и проанализированы сильные корреляции |
| **Визуализация** | 20% | Минимум 5 различных информативных графиков с правильным оформлением |
| **Выявление аномалий** | 10% | Найдены выбросы, проверены логические ошибки и дубликаты |
| **Выводы и рекомендации** | 10% | Содержательные выводы, опирающиеся на результаты анализа |

**Бонусные баллы (+15%):**
- **+5%**: Использованы дополнительные методы анализа (например, pairplot, violin plots, stripplot)
- **+5%**: Проведен углубленный анализ подгрупп данных (сравнение характеристик по категориям)
- **+5%**: Создана интерактивная визуализация (plotly) или дашборд

**Штрафные баллы:**
- **-10%**: Код не выполняется или содержит критические ошибки
- **-5%**: Отсутствуют комментарии и интерпретация результатов
- **-5%**: Визуализации без подписей или нечитаемые

---

## Полезные библиотеки

### Основные библиотеки
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Настройки для красивых графиков
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12
sns.set_style("whitegrid")
sns.set_palette("husl")

# Для отображения всех столбцов в pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
```

### Дополнительные библиотеки (для бонусов)
```python
# Интерактивные визуализации
import plotly.express as px
import plotly.graph_objects as go

# Расширенная визуализация
from pandas.plotting import scatter_matrix

# Продвинутая визуализация отсутствующих данных
import missingno as msno
```

---

## Часто задаваемые вопросы

**Q: Что делать, если данных слишком много для визуализации?**  
A: Используйте случайную выборку: `df.sample(n=5000)` или агрегируйте данные.

**Q: Как быть с категориальными переменными с сотнями уникальных значений?**  
A: Анализируйте топ-10 или топ-20 категорий, остальные объедините в "Другое".

**Q: Нужно ли удалять выбросы на этом этапе?**  
A: Нет, на этапе EDA только **выявляем** и **анализируем** выбросы. Решение об удалении принимается позже.

**Q: Что делать с пропусками?**  
A: Пока только анализируем. Стратегию заполнения разработаем в следующих заданиях.

**Q: Сколько всего визуализаций нужно создать?**  
A: Минимум 5 различных типов графиков (гистограммы, boxplot, heatmap, scatter, bar chart). Качество важнее количества.

**Q: Можно ли использовать готовые функции для EDA (pandas-profiling)?**  
A: Для быстрого ознакомления — да, но основное задание нужно выполнить вручную, чтобы разобраться в методах.

**Q: Как определить, что корреляция "сильная"?**  
A: Общепринятое правило: |r| > 0.7 — сильная, 0.4 < |r| < 0.7 — средняя, |r| < 0.4 — слабая.

---

## Советы по выполнению

1. **Начните с простого** — сначала базовая статистика, потом визуализации
2. **Комментируйте каждый шаг** — объясняйте, что делаете и почему
3. **Обращайте внимание на масштабы** — иногда нужна логарифмическая шкала
4. **Используйте цвет осмысленно** — для выделения важного, не для украшения
5. **Не игнорируйте "скучные" результаты** — отсутствие корреляции тоже важная находка
6. **Проверяйте предположения** — если что-то выглядит странно, разберитесь почему
7. **Сохраняйте важные графики** — используйте `plt.savefig('figure.png', dpi=300)`
8. **Итерируйте** — вернитесь к анализу после создания визуализаций

Помните: цель EDA — **понять данные**, а не просто выполнить набор команд. Будьте любопытны!

---

Удачи в анализе! Если возникнут технические трудности или вопросы по интерпретации результатов, обращайтесь к преподавателю.